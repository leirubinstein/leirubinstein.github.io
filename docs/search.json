[
  {
    "objectID": "nature-photography.html",
    "href": "nature-photography.html",
    "title": "Nature Photography",
    "section": "",
    "text": "Kelp forests at Santa Cruz Island, Channel Islands National Park\n\n\n\n\n\nDolphin breaching water in the Santa Barbara Channel, CA\n\n\n\n\n\nDolphins in the Santa Barbara Channel, CA\n\n\n\n\n\nSchool of sardines in the Santa Barbara Channel, CA\n\n\n\n\n\nMount San Jacinto, Palm Springs, CA\n\n\n\n\n\nWindmills outside of Palm Springs, CA\n\n\n\n\n\nSunset from More Mesa, Santa Barbara, CA\n\n\n\n\n\nUCSB Lagoon, Santa Barbara, CA\n\n\n\n\n\nDead Sea sunrise from Masada, Israel"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi there! My name is Leilanie Rubinstein, and Iâ€™m recent graduate of the Bren School of Environmental Science & Management in Santa Barbara, California. This is my personal website and data science blog (along with a few photos Iâ€™ve taken)."
  },
  {
    "objectID": "about.html#welcome",
    "href": "about.html#welcome",
    "title": "About Me",
    "section": "",
    "text": "Hi there! My name is Leilanie Rubinstein, and Iâ€™m recent graduate of the Bren School of Environmental Science & Management in Santa Barbara, California. This is my personal website and data science blog (along with a few photos Iâ€™ve taken)."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\nIâ€™m from Palm Springs, CA, a desert town in the Coachella Valley of Southern California.\n\n\n\nWindmills outside of Palm Springs\n\n\nGrowing up in a place where the summers are so hot that the asphalt begins to melt primed me to care about the impacts of climate change from an early age. Extreme heat and drought, among other consequences, affect socioeconomically disadvantaged populations the most.\n\n\n\n\n\n\nInteractive map\nMy motivation for pursuing a career in environmental data science stems from wanting to understand how we can better serve our communities and ecosystems impacted by climate change. I strongly believe that data-driven solutions promote better public policy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leilanie Rubinstein",
    "section": "",
    "text": "I earned my Bachelor of Arts in Political Science and Environmental Studies from the University of California, Santa Barbara in 2021. Inspired by Santa Barbaraâ€™s legacy of environmental activism, I have dedicated myself to running local and federal campaigns with social justice and conservation-driven values for the past seven years. Following my undergraduate degree, I worked at an environmental law firm, specializing in litigation and regulatory disputes relating to remediation of Superfund sites. My professional experience includes roles in political organizing, legislation, and environmental law, where I honed skills in public communication, community engagement, and environmental policy analysis.\nAs a recent graduate with a Master of Environmental Data Science from the Bren School of Environmental Science & Management, I aspire to work in environmental economics or policy research, or for local government. I aim to leverage remote sensing and machine learning techniques to develop solutions for resource management and climate change mitigation, promoting sustainable practices and enhancing climate resilience locally and globally.\nIn my free time, I enjoy cycling, gaming, and exploring nature and the night sky through my camera lens. Check out some of my photography here."
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "As global demand for sustainable protein grows, marine aquaculture offers an alternative to land-based meat production. Gentry et al. found through mapping potential for marine aquaculture using multiple constraints, that global seafood demand could be met using less than 0.015% of global ocean area.\nThis exercise determines which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture to several species of oysters, and develops a function for visualizing suitability based on a single speciesâ€”Pteria sterna, the Pacific winged oysterâ€”in this case.\n\n\nshow code\n# Load libraries\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tmap)\nlibrary(testthat)\n\n\n\n\nshow code\n# Source external script that defines a custom function for aquaculture suitability\nsource(here::here(\"posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.R\"))\n\n\nThis function will also be provided below as part of hwk4.qmd.\n\n\n\n\n\nshow code\n# Read in Sea Surface Temperature, Bathymetry, and EEZ data\nfiles &lt;- list.files(here::here(\"posts/2024-12-24-aquaculture-suitability/data\"), pattern = \"average*\", full.names = TRUE)\nsst &lt;- terra::rast(files)\nnames(sst) &lt;- c(2008, 2009, 2010, 2011, 2012)\n\nif (nlyr(sst) == 0) {\n  stop(\"No layers found in SST data\")\n}\n\ndepth &lt;- terra::rast(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"depth.tif\")) %&gt;%\n  project(., y = crs(sst))\n\nwc_eez &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"wc_regions_clean.shp\"), quiet = TRUE) %&gt;%\n  st_transform(., crs = crs(sst))\n\n\n\n\nshow code\n# Check that the CRSs match for the datasets\ntestthat::test_that(\"Coordinate reference systems match\", {\n  expect_equal(crs(sst), crs(depth))\n  expect_equal(crs(depth), crs(wc_eez))\n})\n\n\nTest passed ðŸŽŠ\n\n\n\n\nshow code\n# Import US state boundaries for plotting\nstates &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"states\", \"cb_2023_us_state_20m.shp\"), quiet = TRUE) %&gt;%\n  filter(NAME %in% c(\"California\", \"Oregon\", \"Washington\", \"Nevada\")) %&gt;%\n  st_transform(., crs = crs(sst)) %&gt;%\n  st_crop(., st_bbox(wc_eez))\n\n\n\n\n\n\n\nshow code\nsst_mean &lt;- sst %&gt;%\n  terra::mean() %&gt;%\n  - 273.15 # Convert from ÂºK to ÂºC\n\ndepth_cropped &lt;- crop(depth, sst_mean)\n\nif (res(depth_cropped)[1] != res(sst_mean)[1]) {\n  warning(sprintf(\"Resolution mismatch\", \n                  res(depth_cropped)[1], res(sst_mean)[1]))\n}\n\ndepth_resample &lt;- resample(depth_cropped, sst_mean, method = \"near\")\n\next(depth_resample) == ext(sst_mean)\n\n\n[1] TRUE\n\n\nshow code\n# Check that the depth and SST rasters match in resolution, extent, and position\ndepth_sst_stack &lt;- c(depth_resample, sst_mean)\n\n\n\n\n\nResearch has shown that oysters need the following conditions for optimal growth:\n\nsea surface temperature: 11-30Â°C\ndepth: 0-70 meters below sea level\n\n\n\nshow code\n# Define reclassification matrices for depth and SST\nrcl_depth &lt;- matrix(c(-Inf, -70, 0,\n                      -70, 0, 1,\n                      0, Inf, 0),\n                    ncol = 3, byrow = TRUE)\nrcl_sst &lt;- matrix(c(-Inf, 11, 0,\n                    11, 30, 1,\n                    30, Inf, 0),\n                  ncol = 3, byrow = TRUE)\n\n# Apply the matrices to the depth and SST rasters, making all cells 0 or 1\ndepth_rcl &lt;- terra::classify(depth_resample, rcl = rcl_depth)\nsst_rcl &lt;- terra::classify(sst_mean, rcl = rcl_sst)\n\n# Find locations that satisfy both SST and depth conditions\nsuitablility &lt;- function(depth_rcl, sst_rcl) {\n  depth_rcl*sst_rcl\n}\nsuitable &lt;- lapp(c(depth_rcl, sst_rcl), fun = suitablility)\n\n\n\n\n\n\n\nshow code\n# Set unsuitable locations to NAs\nsuitable[suitable == 0] &lt;- NA\n\n# Find the total suitable area within each EEZ\neez_area &lt;- terra::cellSize(suitable, mask = T, unit = \"km\")\nsuitable_eez_area &lt;- exactextractr::exact_extract(\n  eez_area, wc_eez, \n  fun = \"sum\", \n  append_cols=c(\"rgn\", \"area_km2\"), \n  progress = FALSE)\n\n# Join to original data to obtain geometries for visualization\nsuitable_eez_join &lt;- left_join(wc_eez, suitable_eez_area)\n\ntm_shape(suitable_eez_join) +\n  tm_fill(col = \"sum\",\n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"Suitable Area (km\\u00b2)\") +\n  tm_text(text = \"rgn\",\n          size = 0.7) +\n  tm_shape(states) +\n  tm_polygons(col = \"#e3d3b8\",\n              border.col = \"#402618\") +\n  tm_shape(wc_eez) +\n  tm_borders(col = \"#402618\") +\n  tm_layout(main.title = paste(\"Suitable Area for Oyster Fisheries \\nin West Coast EEZ\"),\n            bg.color = \"#e8ebea\",\n            legend.bg.color = \"#e3d3b8\",\n            legend.position = c(0.61, 0.85)) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(position = c(\"right\", \"bottom\"),\n             size = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the provided depth, sea surface temperature, West Coast Exclusive Economic Zones, and states data, we can conduct a suitability analysis and produce a suitability map for any species of interest for marine aquaculture given their sea surface temperature range, depth range and species name.\n\n\nshow code\nsuitable_aquaculture &lt;- function(min_temp, max_temp, min_depth, max_depth, species) {\n  # Read and prepare data\n  files &lt;- list.files(here::here(\"posts/2024-12-24-aquaculture-suitability/data\"), pattern = \"average*\", full.names = TRUE)\n  sst &lt;- terra::rast(files)\n  names(sst) &lt;- c(2008, 2009, 2010, 2011, 2012)\n  \n  # Import bathymetry data\n  depth &lt;- terra::rast(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"depth.tif\"))\n  \n  # Import EEZ and state boundaries\n  wc_eez &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"wc_regions_clean.shp\"), quiet = TRUE) %&gt;%\n    st_transform(., crs = crs(sst))\n  \n  states &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"states\", \"cb_2023_us_state_20m.shp\"), quiet = TRUE) %&gt;%\n    filter(NAME %in% c(\"California\", \"Oregon\", \"Washington\", \"Nevada\")) %&gt;%\n    st_transform(., crs = crs(sst)) %&gt;%\n    st_crop(., st_bbox(wc_eez))\n  \n  # Process SST data\n  sst_mean &lt;- sst %&gt;%\n    terra::mean() %&gt;%\n    - 273.15  # Convert from Kelvin to Celsius\n  \n  # Process depth data\n  depth &lt;- project(depth, y = crs(sst))\n  depth_cropped &lt;- crop(depth, sst_mean)\n  depth_resample &lt;- resample(depth_cropped, sst_mean, method = \"near\")\n  \n  # Create reclassification matrices\n  rcl_depth &lt;- matrix(c(-Inf, -max_depth, 0,\n                        -max_depth, min_depth, 1,\n                        min_depth, Inf, 0),\n                      ncol = 3, byrow = TRUE)\n  rcl_sst &lt;- matrix(c(-Inf, min_temp, 0,\n                      min_temp, max_temp, 1,\n                      max_temp, Inf, 0),\n                    ncol = 3, byrow = TRUE)\n  \n  # Apply matrices to depth and SST rasters\n  depth_rcl &lt;- terra::classify(depth_resample, rcl = rcl_depth)\n  sst_rcl &lt;- terra::classify(sst_mean, rcl = rcl_sst)\n  \n  # Find locations that satisfy both SST and depth conditions\n  suitablility &lt;- function(depth_rcl, sst_rcl) {\n    depth_rcl*sst_rcl\n  }\n  suitable &lt;- lapp(c(depth_rcl, sst_rcl), fun = suitablility)\n  \n  # Set unsuitable locations to NA\n  suitable[suitable == 0] &lt;- NA\n  \n  # Find the total suitable area within each EEZ\n  eez_area &lt;- terra::cellSize(suitable, mask = T, unit = \"km\")\n  suitable_eez_area &lt;- exactextractr::exact_extract(eez_area, wc_eez, \n                                                    fun = \"sum\", \n                                                    append_cols=c(\"rgn\", \"area_km2\"), \n                                                    progress = FALSE)\n  \n  # Join to original data to obtain geometries for visualization\n  suitable_eez_join &lt;- left_join(wc_eez, suitable_eez_area, by = join_by(rgn, area_km2))\n  \n  # Visualize suitable area\n  tm_shape(suitable_eez_join) +\n    tm_fill(col = \"sum\",\n            style = \"pretty\",\n            palette = \"Blues\",\n            title = \"Suitable Area (km\\u00b2)\") +\n    tm_text(text = \"rgn\",\n            size = 0.7) +\n    tm_shape(states) +\n    tm_polygons(col = \"#e3d3b8\",\n                border.col = \"#402618\") +\n    tm_shape(wc_eez) +\n    tm_borders(col = \"#402618\") +\n    tm_layout(main.title = paste(\"Suitable Area for\", \n                                 species,\n                                 \"\\nFisheries in West Coast EEZ\"),\n              bg.color = \"#e8ebea\",\n              legend.bg.color = \"#e3d3b8\",\n              legend.position = c(0.61, 0.85)) +\n    tm_scale_bar(position = c(\"left\", \"bottom\")) +\n    tm_compass(position = c(\"right\", \"bottom\"),\n               size = 2)\n}\n\n\n\n\n\nPteria sterna or the Pacific wing-oyster is an eastern Pacific oyster of commercial value.\nConditions for optimal growth:\n\nsea surface temperature: 10-30Â°C\ndepth: 3-26 meters below sea level\n\n\n\nshow code\nsuitable_aquaculture(10, 30, 3, 26, \"Pteria sterna\")\n\n\n\n|---------|---------|---------|---------|\n========================================="
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#background",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#background",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "As global demand for sustainable protein grows, marine aquaculture offers an alternative to land-based meat production. Gentry et al. found through mapping potential for marine aquaculture using multiple constraints, that global seafood demand could be met using less than 0.015% of global ocean area.\nThis exercise determines which Exclusive Economic Zones (EEZ) on the West Coast of the US are best suited to developing marine aquaculture to several species of oysters, and develops a function for visualizing suitability based on a single speciesâ€”Pteria sterna, the Pacific winged oysterâ€”in this case.\n\n\nshow code\n# Load libraries\nlibrary(terra)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tmap)\nlibrary(testthat)\n\n\n\n\nshow code\n# Source external script that defines a custom function for aquaculture suitability\nsource(here::here(\"posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.R\"))\n\n\nThis function will also be provided below as part of hwk4.qmd."
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#prepare-data",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#prepare-data",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "show code\n# Read in Sea Surface Temperature, Bathymetry, and EEZ data\nfiles &lt;- list.files(here::here(\"posts/2024-12-24-aquaculture-suitability/data\"), pattern = \"average*\", full.names = TRUE)\nsst &lt;- terra::rast(files)\nnames(sst) &lt;- c(2008, 2009, 2010, 2011, 2012)\n\nif (nlyr(sst) == 0) {\n  stop(\"No layers found in SST data\")\n}\n\ndepth &lt;- terra::rast(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"depth.tif\")) %&gt;%\n  project(., y = crs(sst))\n\nwc_eez &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"wc_regions_clean.shp\"), quiet = TRUE) %&gt;%\n  st_transform(., crs = crs(sst))\n\n\n\n\nshow code\n# Check that the CRSs match for the datasets\ntestthat::test_that(\"Coordinate reference systems match\", {\n  expect_equal(crs(sst), crs(depth))\n  expect_equal(crs(depth), crs(wc_eez))\n})\n\n\nTest passed ðŸŽŠ\n\n\n\n\nshow code\n# Import US state boundaries for plotting\nstates &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"states\", \"cb_2023_us_state_20m.shp\"), quiet = TRUE) %&gt;%\n  filter(NAME %in% c(\"California\", \"Oregon\", \"Washington\", \"Nevada\")) %&gt;%\n  st_transform(., crs = crs(sst)) %&gt;%\n  st_crop(., st_bbox(wc_eez))"
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#process-data",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#process-data",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "show code\nsst_mean &lt;- sst %&gt;%\n  terra::mean() %&gt;%\n  - 273.15 # Convert from ÂºK to ÂºC\n\ndepth_cropped &lt;- crop(depth, sst_mean)\n\nif (res(depth_cropped)[1] != res(sst_mean)[1]) {\n  warning(sprintf(\"Resolution mismatch\", \n                  res(depth_cropped)[1], res(sst_mean)[1]))\n}\n\ndepth_resample &lt;- resample(depth_cropped, sst_mean, method = \"near\")\n\next(depth_resample) == ext(sst_mean)\n\n\n[1] TRUE\n\n\nshow code\n# Check that the depth and SST rasters match in resolution, extent, and position\ndepth_sst_stack &lt;- c(depth_resample, sst_mean)"
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#find-suitable-locations-for-marine-aquaculture",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#find-suitable-locations-for-marine-aquaculture",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "Research has shown that oysters need the following conditions for optimal growth:\n\nsea surface temperature: 11-30Â°C\ndepth: 0-70 meters below sea level\n\n\n\nshow code\n# Define reclassification matrices for depth and SST\nrcl_depth &lt;- matrix(c(-Inf, -70, 0,\n                      -70, 0, 1,\n                      0, Inf, 0),\n                    ncol = 3, byrow = TRUE)\nrcl_sst &lt;- matrix(c(-Inf, 11, 0,\n                    11, 30, 1,\n                    30, Inf, 0),\n                  ncol = 3, byrow = TRUE)\n\n# Apply the matrices to the depth and SST rasters, making all cells 0 or 1\ndepth_rcl &lt;- terra::classify(depth_resample, rcl = rcl_depth)\nsst_rcl &lt;- terra::classify(sst_mean, rcl = rcl_sst)\n\n# Find locations that satisfy both SST and depth conditions\nsuitablility &lt;- function(depth_rcl, sst_rcl) {\n  depth_rcl*sst_rcl\n}\nsuitable &lt;- lapp(c(depth_rcl, sst_rcl), fun = suitablility)"
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#determine-the-most-suitable-eez",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#determine-the-most-suitable-eez",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "show code\n# Set unsuitable locations to NAs\nsuitable[suitable == 0] &lt;- NA\n\n# Find the total suitable area within each EEZ\neez_area &lt;- terra::cellSize(suitable, mask = T, unit = \"km\")\nsuitable_eez_area &lt;- exactextractr::exact_extract(\n  eez_area, wc_eez, \n  fun = \"sum\", \n  append_cols=c(\"rgn\", \"area_km2\"), \n  progress = FALSE)\n\n# Join to original data to obtain geometries for visualization\nsuitable_eez_join &lt;- left_join(wc_eez, suitable_eez_area)\n\ntm_shape(suitable_eez_join) +\n  tm_fill(col = \"sum\",\n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"Suitable Area (km\\u00b2)\") +\n  tm_text(text = \"rgn\",\n          size = 0.7) +\n  tm_shape(states) +\n  tm_polygons(col = \"#e3d3b8\",\n              border.col = \"#402618\") +\n  tm_shape(wc_eez) +\n  tm_borders(col = \"#402618\") +\n  tm_layout(main.title = paste(\"Suitable Area for Oyster Fisheries \\nin West Coast EEZ\"),\n            bg.color = \"#e8ebea\",\n            legend.bg.color = \"#e3d3b8\",\n            legend.position = c(0.61, 0.85)) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(position = c(\"right\", \"bottom\"),\n             size = 2)"
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#single-species-analysis-function",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#single-species-analysis-function",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "Using the provided depth, sea surface temperature, West Coast Exclusive Economic Zones, and states data, we can conduct a suitability analysis and produce a suitability map for any species of interest for marine aquaculture given their sea surface temperature range, depth range and species name.\n\n\nshow code\nsuitable_aquaculture &lt;- function(min_temp, max_temp, min_depth, max_depth, species) {\n  # Read and prepare data\n  files &lt;- list.files(here::here(\"posts/2024-12-24-aquaculture-suitability/data\"), pattern = \"average*\", full.names = TRUE)\n  sst &lt;- terra::rast(files)\n  names(sst) &lt;- c(2008, 2009, 2010, 2011, 2012)\n  \n  # Import bathymetry data\n  depth &lt;- terra::rast(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"depth.tif\"))\n  \n  # Import EEZ and state boundaries\n  wc_eez &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"wc_regions_clean.shp\"), quiet = TRUE) %&gt;%\n    st_transform(., crs = crs(sst))\n  \n  states &lt;- st_read(here::here(\"posts/2024-12-24-aquaculture-suitability/data\", \"states\", \"cb_2023_us_state_20m.shp\"), quiet = TRUE) %&gt;%\n    filter(NAME %in% c(\"California\", \"Oregon\", \"Washington\", \"Nevada\")) %&gt;%\n    st_transform(., crs = crs(sst)) %&gt;%\n    st_crop(., st_bbox(wc_eez))\n  \n  # Process SST data\n  sst_mean &lt;- sst %&gt;%\n    terra::mean() %&gt;%\n    - 273.15  # Convert from Kelvin to Celsius\n  \n  # Process depth data\n  depth &lt;- project(depth, y = crs(sst))\n  depth_cropped &lt;- crop(depth, sst_mean)\n  depth_resample &lt;- resample(depth_cropped, sst_mean, method = \"near\")\n  \n  # Create reclassification matrices\n  rcl_depth &lt;- matrix(c(-Inf, -max_depth, 0,\n                        -max_depth, min_depth, 1,\n                        min_depth, Inf, 0),\n                      ncol = 3, byrow = TRUE)\n  rcl_sst &lt;- matrix(c(-Inf, min_temp, 0,\n                      min_temp, max_temp, 1,\n                      max_temp, Inf, 0),\n                    ncol = 3, byrow = TRUE)\n  \n  # Apply matrices to depth and SST rasters\n  depth_rcl &lt;- terra::classify(depth_resample, rcl = rcl_depth)\n  sst_rcl &lt;- terra::classify(sst_mean, rcl = rcl_sst)\n  \n  # Find locations that satisfy both SST and depth conditions\n  suitablility &lt;- function(depth_rcl, sst_rcl) {\n    depth_rcl*sst_rcl\n  }\n  suitable &lt;- lapp(c(depth_rcl, sst_rcl), fun = suitablility)\n  \n  # Set unsuitable locations to NA\n  suitable[suitable == 0] &lt;- NA\n  \n  # Find the total suitable area within each EEZ\n  eez_area &lt;- terra::cellSize(suitable, mask = T, unit = \"km\")\n  suitable_eez_area &lt;- exactextractr::exact_extract(eez_area, wc_eez, \n                                                    fun = \"sum\", \n                                                    append_cols=c(\"rgn\", \"area_km2\"), \n                                                    progress = FALSE)\n  \n  # Join to original data to obtain geometries for visualization\n  suitable_eez_join &lt;- left_join(wc_eez, suitable_eez_area, by = join_by(rgn, area_km2))\n  \n  # Visualize suitable area\n  tm_shape(suitable_eez_join) +\n    tm_fill(col = \"sum\",\n            style = \"pretty\",\n            palette = \"Blues\",\n            title = \"Suitable Area (km\\u00b2)\") +\n    tm_text(text = \"rgn\",\n            size = 0.7) +\n    tm_shape(states) +\n    tm_polygons(col = \"#e3d3b8\",\n                border.col = \"#402618\") +\n    tm_shape(wc_eez) +\n    tm_borders(col = \"#402618\") +\n    tm_layout(main.title = paste(\"Suitable Area for\", \n                                 species,\n                                 \"\\nFisheries in West Coast EEZ\"),\n              bg.color = \"#e8ebea\",\n              legend.bg.color = \"#e3d3b8\",\n              legend.position = c(0.61, 0.85)) +\n    tm_scale_bar(position = c(\"left\", \"bottom\")) +\n    tm_compass(position = c(\"right\", \"bottom\"),\n               size = 2)\n}"
  },
  {
    "objectID": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#test-function-for-single-oyster-species",
    "href": "posts/2024-12-24-aquaculture-suitability/suitable-aquaculture.html#test-function-for-single-oyster-species",
    "title": "Aquaculture Suitability Map",
    "section": "",
    "text": "Pteria sterna or the Pacific wing-oyster is an eastern Pacific oyster of commercial value.\nConditions for optimal growth:\n\nsea surface temperature: 10-30Â°C\ndepth: 3-26 meters below sea level\n\n\n\nshow code\nsuitable_aquaculture(10, 30, 3, 26, \"Pteria sterna\")\n\n\n\n|---------|---------|---------|---------|\n========================================="
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "# Load all packages\nlibrarian::shelf(\n  tidyverse,\n  sf,\n  terra,\n  stars,\n  raster,\n  tidyterra,\n  tmap,\n  viridis,\n  patchwork,\n  scales,\n  kableExtra,\n  janitor,\n  here,\n  testthat\n)\n\n\n# Import night lights data\nnight_lights1 &lt;- terra::rast(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\"data\", \"VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\nnight_lights2 &lt;- terra::rast(here::here(\"posts/2024-12-09-houston-blackout-visualization\", \"data\", \"VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\nnight_lights3 &lt;- terra::rast(here::here(\"posts/2024-12-09-houston-blackout-visualization\", \"data\", \"VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\nnight_lights4 &lt;- terra::rast(here::here(\"posts/2024-12-09-houston-blackout-visualization\", \"data\", \"VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n\n# Merge and process night lights data, cropping to the Houston area\nhouston_extent &lt;- extent(c(-96.5, -94.5, 29, 30.5))\n\n# Check if CRS matches before merging\nif (crs(night_lights1) != crs(night_lights2)) {\n  stop(\"CRS does not match between night_lights1 and night_lights2\")\n}\nif (crs(night_lights3) != crs(night_lights4)) {\n  stop(\"CRS does not match between night_lights3 and night_lights4\")\n}\n\nnight_lights_before &lt;- terra::merge(night_lights1, night_lights2) %&gt;%\n  crop(houston_extent)\nnight_lights_after &lt;- terra::merge(night_lights3, night_lights4) %&gt;%\n  crop(houston_extent)\n\n# Create blackout mask\nnight_lights_change &lt;- night_lights_after - night_lights_before\nnight_lights_change[night_lights_change &gt; -200] &lt;- NA\n\n\n# Vectorize blackout mask\nnight_lights_change_poly &lt;- as.polygons(night_lights_change) %&gt;%\n  st_as_sf() %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\n\n# Import infrastructure data\nroads &lt;- read_sf(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                            \"data\", \"gis_osm_roads_free_1.gpkg\"), \n                 query = \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\") %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\nhouses &lt;- read_sf(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                             \"data\", \"gis_osm_buildings_a_free_1.gpkg\"),\n                  query = \"SELECT * FROM gis_osm_buildings_a_free_1 \n                         WHERE (type IS NULL AND name IS NULL)\n                         OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\") %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\n\n# Create road buffer and exclude highways from blackout mask\nroads_buffer &lt;- st_buffer(roads, dist = 200)\nblackouts_200m &lt;- st_difference(night_lights_change_poly, st_union(roads_buffer))\n\ntm_shape(blackouts_200m) +\n  tm_polygons() + \n  tm_layout(main.title = \"Blackout Mask\")\n\n\n\n\n\n\n\n\n\n\n\n# Calculate common min and max for both rasters after log1p transformation\nmin_val &lt;- min(c(values(log1p(night_lights_before)), \n                 values(log1p(night_lights_after))), na.rm = TRUE)\nmax_val &lt;- max(c(values(log1p(night_lights_before)), \n                 values(log1p(night_lights_after))), na.rm = TRUE)\n\np1 &lt;- ggplot() +\n geom_spatraster(data = log1p(night_lights_before)) +\n scale_fill_viridis_c(name = \"Light Intensity\\nlog1p(nW/cmÂ²/sr)\", \n                      option = \"plasma\",\n                      limits = c(min_val, max_val),\n                      na.value = \"transparent\",\n                      guide = guide_colorbar(\n                        title.position = \"top\",\n                        title.hjust = 0.5,\n                        barwidth = 1.2,\n                        barheight = 15,\n                        frame.colour = \"black\",\n                        frame.linewidth = 0.5,\n                        ticks.colour = \"black\"\n                      )) +\n labs(title = \"Nighttime Light Intensity\",\n      subtitle = \"Before Storm (February 7, 2021)\") +\n theme_void() +\n theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n       plot.subtitle = element_text(hjust = 0.5, size = 10),\n       legend.position = \"right\")\n\np2 &lt;- ggplot() +\n geom_spatraster(data = log1p(night_lights_after)) +\n scale_fill_viridis_c(name = \"Light Intensity\\nlog1p(nW/cmÂ²/sr)\", \n                      option = \"plasma\",\n                      limits = c(min_val, max_val),\n                      na.value = \"transparent\") +\n labs(title = \"Nighttime Light Intensity\",\n      subtitle = \"After Storm (February 16, 2021)\") +\n theme_void() +\n theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n       plot.subtitle = element_text(hjust = 0.5, size = 10),\n       legend.position = \"none\")\n\n# Combine plots with vertical legend between them\np1 + p2 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n# houses_filtered &lt;- houses[blackouts_200m, ]\n\n# Convert blackouts to a raster/stars object\nblackouts_raster &lt;- st_rasterize(blackouts_200m, \n                                 dx = 100, \n                                 dy = 100)\n\n# Convert houses to points (centroids) for faster processing\nhouses_points &lt;- st_centroid(houses)\n\n# Extract values at house locations\nhouse_values &lt;- st_extract(blackouts_raster, houses_points)\n\n# Filter houses based on extracted values\nhouses_filtered &lt;- houses[!is.na(house_values[[1]]), ]\n\ntm_shape(houses_filtered) +\n  tm_dots(col = \"#CBC3E3\") +\n  tm_compass(type = \"8star\",\n             size = 0.8) +\n  tm_scale_bar() +\n  tm_layout(main.title = \"Homes in Houston that Lost Power\")\n\n\n\n\n\n\n\n\n\n\n\n\nnrow(houses_filtered)\n\n[1] 153745\n\nhouses_count &lt;- houses_filtered %&gt;%\n  group_by(type) %&gt;%\n  summarize(count = n()) %&gt;%\n  ungroup() %&gt;%\n  st_drop_geometry()\n\n# Check that the sum of buildings adds up to the number of rows in `houses_filtered` \ntestthat::expect_equal(sum(houses_count$count), nrow(houses_filtered))\n\nhouses_count_table &lt;- houses_count %&gt;%\n  kbl(caption = \"Number of homes that experienced power loss (total = 157967)\") %&gt;%\n  kable_classic(html_font = \"Cambria\")\n\nhouses_count_table\n\n\nNumber of homes that experienced power loss (total = 157967)\n\n\ntype\ncount\n\n\n\n\napartments\n1072\n\n\ndetached\n344\n\n\nhouse\n19338\n\n\nresidential\n1324\n\n\nstatic_caravan\n78\n\n\nNA\n131589\n\n\n\n\n\n\n\n\n\n\n\n# Import census data\nacs_texas &lt;- st_read(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                                \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                     layer =\"ACS_2019_5YR_TRACT_48_TEXAS\")\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `/Users/leilanie/git/leirubinstein.github.io/posts/2024-12-09-houston-blackout-visualization/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\nacs_income &lt;- st_read(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                                 \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                      layer = \"X19_INCOME\")\n\nReading layer `X19_INCOME' from data source \n  `/Users/leilanie/git/leirubinstein.github.io/posts/2024-12-09-houston-blackout-visualization/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n# Join the median household income from the previous 12 months to the census tract geometries\nacs &lt;- left_join(acs_income, acs_texas, join_by(GEOID == GEOID_Data)) %&gt;%\n  st_as_sf() %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\n# Create a bounding box for the Houston area\nhouston_bbox &lt;- st_bbox(c(xmin = -96.5, xmax = -94.5, \n                          ymin = 29, ymax = 30.5)) %&gt;%\n  st_as_sfc() %&gt;%\n  st_set_crs(4326) %&gt;%\n  st_transform(st_crs(acs))\n\n# Filter to Houston area census tracts\nhouston_tracts &lt;- acs[houston_bbox, ]\n\n# Identify census tracts that contained homes that experienced blackouts\nhouston_tracts_blackouts &lt;- st_filter(houston_tracts, houses_filtered)\n\ntm_shape(houston_tracts) +\n  tm_polygons(col = \"white\", border.col = \"gray\") +\n  tm_shape(houston_tracts_blackouts) +\n  tm_polygons(col = \"#CBC3E3\", border.col = \"black\", lwd = 0.5) +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\"), size = 0.8) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_layout(main.title = \"Census Tracts in Houston that Experienced Blackouts\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n# Find census tracts that did NOT experience blackouts\nhouston_non_blackout &lt;- houston_tracts %&gt;%\n  filter(!GEOID %in% houston_tracts_blackouts$GEOID)\n\n# Label tracts by power status\nhouston_tracts_blackouts$status &lt;- \"Lost Power\"\nhouston_non_blackout$status &lt;- \"Did Not Lose Power\"\ncombined_tracts &lt;- rbind(houston_tracts_blackouts, houston_non_blackout)\n\n# Reorder the factor levels to put \"Did Not Lose Power\" first\ncombined_tracts$status &lt;- factor(combined_tracts$status, \n                                levels = c(\"Did Not Lose Power\", \"Lost Power\"))\n\n# Plot combined tracts\nggplot(combined_tracts, aes(y = status, x = B19013e1, fill = status)) +\n  geom_boxplot(alpha = 0.8, outlier.alpha = 0.6) +\n  labs(title = \"Distribution of Median Household Income by Power Loss Status\",\n       subtitle = \"Houston Census Tracts During February 2021 Winter Storm\",\n       x = \"Median Household Income (2011 inflation-adjusted dollars)\",\n       y = \"Power Status\",\n       fill = \"Power Status\") +\n  scale_fill_manual(values = c(\"Did Not Lose Power\" = \"lightgreen\", \n                               \"Lost Power\" = \"#CBC3E3\")) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 11),\n        axis.text = element_text(size = 10))\n\n\n\n\n\n\n\n\n\n\n\nI found that median household income was slightly higher in the census tracts that lost power vs.Â those that did not lose power. Census tracts on the outskirts of the Houston area generally experienced less power loss, but most of the majority of tracts were still affected. According to Busby et al., all socio-economic groups were affected, but the effects of the storm were harder on low-income families, who live in older, poorly insulated homes and have limited access to resources for repairs and physical health after the storm.\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nNight Lights\nNASA Earth Data (2024). Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC)\nLevel-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC).\n\n\nInfrastructure Data\nPlanet OSM (2024). Retrieved from https://download.geofabrik.de/\nLink\n\n\nCensus Data\nU.S. Census Bureau. (2019). American Community Survey 1-year Public Use Microdata Samples. Retrieved from https://www.census.gov/programs-surveys/acs/news/data-releases.2019.html#list-tab-1133175109\n[Link] (https://www.census.gov/programs-surveys/acs/news/data-releases.2019.html#list-tab-1133175109)\n\n\nUnderstanding the 2021 Texas Blackout\nBusby, J. W., Baker, K., Bazilian, M. D., Gilbert, A. Q., Grubert, E., Rai, V., Rhodes, J. D., Shidore, S., Smith, C. A., & Webber, M. E. (2021). Cascading risks: Understanding the 2021 winter blackout in Texas. Energy Research & Social Science, 77, 102106. https://doi.org/10.1016/j.erss.2021.102106\nCascading risks: Understanding the 2021 winter blackout in Texas"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-set-of-maps-comparing-night-light-intensities-before-and-after-the-storms",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-set-of-maps-comparing-night-light-intensities-before-and-after-the-storms",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "# Calculate common min and max for both rasters after log1p transformation\nmin_val &lt;- min(c(values(log1p(night_lights_before)), \n                 values(log1p(night_lights_after))), na.rm = TRUE)\nmax_val &lt;- max(c(values(log1p(night_lights_before)), \n                 values(log1p(night_lights_after))), na.rm = TRUE)\n\np1 &lt;- ggplot() +\n geom_spatraster(data = log1p(night_lights_before)) +\n scale_fill_viridis_c(name = \"Light Intensity\\nlog1p(nW/cmÂ²/sr)\", \n                      option = \"plasma\",\n                      limits = c(min_val, max_val),\n                      na.value = \"transparent\",\n                      guide = guide_colorbar(\n                        title.position = \"top\",\n                        title.hjust = 0.5,\n                        barwidth = 1.2,\n                        barheight = 15,\n                        frame.colour = \"black\",\n                        frame.linewidth = 0.5,\n                        ticks.colour = \"black\"\n                      )) +\n labs(title = \"Nighttime Light Intensity\",\n      subtitle = \"Before Storm (February 7, 2021)\") +\n theme_void() +\n theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n       plot.subtitle = element_text(hjust = 0.5, size = 10),\n       legend.position = \"right\")\n\np2 &lt;- ggplot() +\n geom_spatraster(data = log1p(night_lights_after)) +\n scale_fill_viridis_c(name = \"Light Intensity\\nlog1p(nW/cmÂ²/sr)\", \n                      option = \"plasma\",\n                      limits = c(min_val, max_val),\n                      na.value = \"transparent\") +\n labs(title = \"Nighttime Light Intensity\",\n      subtitle = \"After Storm (February 16, 2021)\") +\n theme_void() +\n theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n       plot.subtitle = element_text(hjust = 0.5, size = 10),\n       legend.position = \"none\")\n\n# Combine plots with vertical legend between them\np1 + p2 + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-map-of-the-homes-in-houston-that-lost-power",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-map-of-the-homes-in-houston-that-lost-power",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "# houses_filtered &lt;- houses[blackouts_200m, ]\n\n# Convert blackouts to a raster/stars object\nblackouts_raster &lt;- st_rasterize(blackouts_200m, \n                                 dx = 100, \n                                 dy = 100)\n\n# Convert houses to points (centroids) for faster processing\nhouses_points &lt;- st_centroid(houses)\n\n# Extract values at house locations\nhouse_values &lt;- st_extract(blackouts_raster, houses_points)\n\n# Filter houses based on extracted values\nhouses_filtered &lt;- houses[!is.na(house_values[[1]]), ]\n\ntm_shape(houses_filtered) +\n  tm_dots(col = \"#CBC3E3\") +\n  tm_compass(type = \"8star\",\n             size = 0.8) +\n  tm_scale_bar() +\n  tm_layout(main.title = \"Homes in Houston that Lost Power\")"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#estimate-the-number-of-homes-in-houston-that-lost-power",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#estimate-the-number-of-homes-in-houston-that-lost-power",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "nrow(houses_filtered)\n\n[1] 153745\n\nhouses_count &lt;- houses_filtered %&gt;%\n  group_by(type) %&gt;%\n  summarize(count = n()) %&gt;%\n  ungroup() %&gt;%\n  st_drop_geometry()\n\n# Check that the sum of buildings adds up to the number of rows in `houses_filtered` \ntestthat::expect_equal(sum(houses_count$count), nrow(houses_filtered))\n\nhouses_count_table &lt;- houses_count %&gt;%\n  kbl(caption = \"Number of homes that experienced power loss (total = 157967)\") %&gt;%\n  kable_classic(html_font = \"Cambria\")\n\nhouses_count_table\n\n\nNumber of homes that experienced power loss (total = 157967)\n\n\ntype\ncount\n\n\n\n\napartments\n1072\n\n\ndetached\n344\n\n\nhouse\n19338\n\n\nresidential\n1324\n\n\nstatic_caravan\n78\n\n\nNA\n131589"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-map-of-the-census-tracts-in-houston-that-lost-power",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#create-a-map-of-the-census-tracts-in-houston-that-lost-power",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "# Import census data\nacs_texas &lt;- st_read(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                                \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                     layer =\"ACS_2019_5YR_TRACT_48_TEXAS\")\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `/Users/leilanie/git/leirubinstein.github.io/posts/2024-12-09-houston-blackout-visualization/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\nacs_income &lt;- st_read(here::here(\"posts/2024-12-09-houston-blackout-visualization\",\n                                 \"data\", \"ACS_2019_5YR_TRACT_48_TEXAS.gdb\"), \n                      layer = \"X19_INCOME\")\n\nReading layer `X19_INCOME' from data source \n  `/Users/leilanie/git/leirubinstein.github.io/posts/2024-12-09-houston-blackout-visualization/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n# Join the median household income from the previous 12 months to the census tract geometries\nacs &lt;- left_join(acs_income, acs_texas, join_by(GEOID == GEOID_Data)) %&gt;%\n  st_as_sf() %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\n# Create a bounding box for the Houston area\nhouston_bbox &lt;- st_bbox(c(xmin = -96.5, xmax = -94.5, \n                          ymin = 29, ymax = 30.5)) %&gt;%\n  st_as_sfc() %&gt;%\n  st_set_crs(4326) %&gt;%\n  st_transform(st_crs(acs))\n\n# Filter to Houston area census tracts\nhouston_tracts &lt;- acs[houston_bbox, ]\n\n# Identify census tracts that contained homes that experienced blackouts\nhouston_tracts_blackouts &lt;- st_filter(houston_tracts, houses_filtered)\n\ntm_shape(houston_tracts) +\n  tm_polygons(col = \"white\", border.col = \"gray\") +\n  tm_shape(houston_tracts_blackouts) +\n  tm_polygons(col = \"#CBC3E3\", border.col = \"black\", lwd = 0.5) +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\"), size = 0.8) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_layout(main.title = \"Census Tracts in Houston that Experienced Blackouts\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            frame = FALSE)"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#compare-the-distributions-of-median-household-income-for-census-tracts-tnihat-did-and-did-not-experience-blackouts",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#compare-the-distributions-of-median-household-income-for-census-tracts-tnihat-did-and-did-not-experience-blackouts",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "# Find census tracts that did NOT experience blackouts\nhouston_non_blackout &lt;- houston_tracts %&gt;%\n  filter(!GEOID %in% houston_tracts_blackouts$GEOID)\n\n# Label tracts by power status\nhouston_tracts_blackouts$status &lt;- \"Lost Power\"\nhouston_non_blackout$status &lt;- \"Did Not Lose Power\"\ncombined_tracts &lt;- rbind(houston_tracts_blackouts, houston_non_blackout)\n\n# Reorder the factor levels to put \"Did Not Lose Power\" first\ncombined_tracts$status &lt;- factor(combined_tracts$status, \n                                levels = c(\"Did Not Lose Power\", \"Lost Power\"))\n\n# Plot combined tracts\nggplot(combined_tracts, aes(y = status, x = B19013e1, fill = status)) +\n  geom_boxplot(alpha = 0.8, outlier.alpha = 0.6) +\n  labs(title = \"Distribution of Median Household Income by Power Loss Status\",\n       subtitle = \"Houston Census Tracts During February 2021 Winter Storm\",\n       x = \"Median Household Income (2011 inflation-adjusted dollars)\",\n       y = \"Power Status\",\n       fill = \"Power Status\") +\n  scale_fill_manual(values = c(\"Did Not Lose Power\" = \"lightgreen\", \n                               \"Lost Power\" = \"#CBC3E3\")) +\n  scale_x_continuous(labels = scales::dollar_format()) +\n  theme_bw() +\n  theme(legend.position = \"none\",\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        axis.title = element_text(size = 11),\n        axis.text = element_text(size = 10))"
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#reflection",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#reflection",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "I found that median household income was slightly higher in the census tracts that lost power vs.Â those that did not lose power. Census tracts on the outskirts of the Houston area generally experienced less power loss, but most of the majority of tracts were still affected. According to Busby et al., all socio-economic groups were affected, but the effects of the storm were harder on low-income families, who live in older, poorly insulated homes and have limited access to resources for repairs and physical health after the storm."
  },
  {
    "objectID": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#data-citations",
    "href": "posts/2024-12-09-houston-blackout-visualization/houston-blackouts.html#data-citations",
    "title": "Houston Blackouts Mapping",
    "section": "",
    "text": "Data\nCitation\nLink\n\n\n\n\nNight Lights\nNASA Earth Data (2024). Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC)\nLevel-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC).\n\n\nInfrastructure Data\nPlanet OSM (2024). Retrieved from https://download.geofabrik.de/\nLink\n\n\nCensus Data\nU.S. Census Bureau. (2019). American Community Survey 1-year Public Use Microdata Samples. Retrieved from https://www.census.gov/programs-surveys/acs/news/data-releases.2019.html#list-tab-1133175109\n[Link] (https://www.census.gov/programs-surveys/acs/news/data-releases.2019.html#list-tab-1133175109)\n\n\nUnderstanding the 2021 Texas Blackout\nBusby, J. W., Baker, K., Bazilian, M. D., Gilbert, A. Q., Grubert, E., Rai, V., Rhodes, J. D., Shidore, S., Smith, C. A., & Webber, M. E. (2021). Cascading risks: Understanding the 2021 winter blackout in Texas. Energy Research & Social Science, 77, 102106. https://doi.org/10.1016/j.erss.2021.102106\nCascading risks: Understanding the 2021 winter blackout in Texas"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "",
    "text": "Link to repo (more content available here!)"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#about",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#about",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "About",
    "text": "About\nPurpose: This analysis explores the 2017 Thomas Fire using two main approaches:\n\nCreating visualizations of the fire perimeter and burn area using Landsat satellite imagery\nAnalyzing the types and distribution of land cover affected by the fire\n\nUsing historical fire perimeter data from CalFire combined with Landsat imagery, we create detailed maps showing both true-color and false-color visualizations of the Thomas Fire area. Here we use historical fire perimeter data from CalFire to obtain the perimeter of the 2017 Thomas Fire. Using provided Landsat imagery and the fire perimeter, we create a map showing the perimeter of the 2017 Thomas Fire in relation to Santa Barbara County.\nWe then analyze land cover data to understand what types of ecosystems were impacted by the fire. Using the GAP/LANDFIRE National Terrestrial Ecosystems data for 2011 from the US Geological Survey (USGS), we calculate the area of each land cover type within the fire perimeter and compare it to the total area of each land cover type in Santa Barbara County.\nHighlights:\n\nCleaning data\nLabel-based selection\nGeospatial file creation\nrioxarray raster file manipulation\nTrue color imagery plotting\nFalse color imagery plotting\nPixel-based land cover calculations\nArea and percentage computations\n\nDataset Descriptions:\nSection 1:\nThe landsat data is a simplified collection of bands from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. This data was retrieved from the Microsoft Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution.\nCaliforniaâ€™s Fire and Resource Assessment Program (FRAP) maintains a comprehensive database of historical fire perimeters throughout the state, covering both public and private lands. This GIS dataset is updated annually through a collaborative effort between FRAP and several federal agencies - the U.S. Forest Service Region 5, Bureau of Land Management, National Park Service, and Fish and Wildlife Service.\nSection 2:\nThe GAP/LANDFIRE National Terrestrial Ecosystems data for 2011 @davidson_gaplandfire_2016, from the US Geological Survey (USGS) is a categorical raster with a 30 m x 30 m pixel resolution representing highly thematically detailed land cover map of the U.S.\nThe class names and corresponding codes have been saved to a separate CSV to simplify access. Further information on how to access the classes directly from the data are available in the MPC catalog.\nPlease see end for references\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rioxarray as rioxr\nfrom shapely import box\nfrom IPython.display import Image"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#data-loading-and-initial-processing",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#data-loading-and-initial-processing",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Data Loading and Initial Processing",
    "text": "Data Loading and Initial Processing\n\n# Load fire perimeter data from a geodatabase file\nfire_perimeter = gpd.read_file(os.path.join(\"data\", \"fire23_1.gdb\"))\nfire_perimeter.head()\n\n# Convert columns to lowercase for consistency\nfire_perimeter.columns = fire_perimeter.columns.str.lower()\n\n\nprint(\n    f\"Fire Perimeter CRS: {fire_perimeter.crs} \\nIs this projected?: {fire_perimeter.crs.is_projected}\"\n)\n\nFire Perimeter CRS: EPSG:3310 \nIs this projected?: True\n\n\nThe fire perimeter data uses EPSG: 3310, which is a projected coordinate reference system specifically designed for California."
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#thomas-fire-perimeter",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#thomas-fire-perimeter",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Thomas Fire Perimeter",
    "text": "Thomas Fire Perimeter\nLetâ€™s create a mask of the Thomas Fire perimeter. First, filter the historical fire perimeters dataset to the 2017 Thomas Fire and then save as a geospatial file in the data/ directory.\n\n# Filter to the 2017 Thomas Fire\nthomasfire = fire_perimeter.loc[\n    (fire_perimeter[\"fire_name\"] == \"THOMAS\") & (fire_perimeter[\"year_\"] == 2017)\n]\n\nCreate a visualization of the Thomas Fire perimeter using geopandas and matplotlib.\n\n# Plot the perimeter\nfig, ax = plt.subplots(figsize=(10, 8))\nax.axis(\"off\")\nthomasfire.plot(\n    ax=ax,\n    color=\"firebrick\",\n)\n\n# Set title\nax.set_title(\"Thomas Fire (2017) Perimeter Mask\")\n\n# Add source information with adjusted position\nplt.figtext(0.01, 0.02, \"Data: CAL FIRE\", ha=\"left\", fontsize=10)\n\n# Adjust layout\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.05)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Save file to the data folder in `.geojson` format.\nthomasfire.to_file(\"data/thomasfire.geojson\", driver=\"GeoJSON\")\n\nGeoJSON is an open standard format that can represent a variety of geometries. It is based on JavaScript Object Notation (JSON), and is commonly used to store spatial data."
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#landsat-data-processing",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#landsat-data-processing",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Landsat Data Processing",
    "text": "Landsat Data Processing\nTo plot true and false color images, we will use the Landsat 8 satellite imagery from January 26, 2018, shortly after the Thomas Fire. The data includes several spectral bands:\n\nRed, Green, Blue (visible light)\nNIR (Near Infrared)\nSWIR (Short-wave Infrared)\n\nThe landsat data is accessed from UCSB Brenâ€™s workbench-1 server at:\n/courses/EDS220/data/hwk4_landsat_data landsat8-2018-01-26-sb-simplified.nc\n\n# Import data\nroot = os.path.join(\"/\", \"courses\", \"EDS220\", \"data\", \"hwk4_landsat_data\")\n\nfp = os.path.join(root, \"landsat8-2018-01-26-sb-simplified.nc\")\n\nlandsat = rioxr.open_rasterio(fp)\n\nLetâ€™s do some preliminary data exploration to understand the structure of the data.\n\n# Check CRS of data and dimensions\nprint(\n    f\"{'Landsat 8 CRS:':&lt;25} {landsat.rio.crs}\\n\"\n    f\"{'Is it projected?':&lt;25} {landsat.rio.crs.is_projected}\\n\"\n    f\"{'Sizes of dimensions:':&lt;25} {dict(landsat.sizes)}\"\n)\n\nLandsat 8 CRS:            EPSG:32611\nIs it projected?          True\nSizes of dimensions:      {'band': 1, 'x': 870, 'y': 731}\n\n\n\nprint(landsat)\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (band: 1, x: 870, y: 731)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0\nData variables:\n    red          (band, y, x) float64 5MB ...\n    green        (band, y, x) float64 5MB ...\n    blue         (band, y, x) float64 5MB ...\n    nir08        (band, y, x) float64 5MB ...\n    swir22       (band, y, x) float64 5MB ...\n\n\nThe landsat dataset has three dimensions: band, x, and y; it has variables for red, green, blue, near infrared 0.8, and short-wave infrared 2.2 bands. The coordinate reference system is EPSG: 32611 (projected).\nThe band dimension makes our raster three dimensional and is unnecessary. We can go ahead and â€œsqueezeâ€ the raster to simplify it:\n\n# Drop the band dimension of the data\nlandsat = landsat.squeeze().drop_vars(\"band\")"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#true-color-image",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#true-color-image",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "True color image",
    "text": "True color image\nUsing the red, green, and blue bands from the landsat data, we can create a true-color visualization of Santa Barbara County.\n\n# Create true color visualization using red, green, blue bands\nlandsat[[\"red\", \"green\", \"blue\"]].to_array().plot.imshow()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nThe plot is black and white; this is because there are a large number of high RGB values from clouds skewing the color scale for the plot.\nPassing the robust=True parameter in plt.imshow prevents outliers from washing out the color scale of the plot. robust=True uses the 2nd and 98th percentiles of the data to compute the color limits. The adjusted output takes into account that the RGB values of the clouds are outliers.\n\n# Set `robust=True` for better color scale when plotting\nlandsat[[\"red\", \"green\", \"blue\"]].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nNow, our image more closely resembles the true color of the landscape."
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#false-color-image",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#false-color-image",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "False color image",
    "text": "False color image\nFalse color imagery is a useful tool for monitoring wildfire impacts. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies. To create a false color image, we will use the short wave infrared, near-infrared, and red bands from the Landsat data.\n\n# Create false color visualization using SWIR, NIR and red bands\nlandsat[[\"swir22\", \"nir08\", \"red\"]].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nTo add the Thomas Fire perimeter we created earlier, import the file and make sure the CRS for landsat matches. This step is crucial because:\n\nDifferent data sources often use different CRS\nMismatched CRS can cause spatial misalignment\nEPSG:3310 (California Albers) is optimal for California analysis\n\n\n# Import the Thomas Fire perimeter\nthomasfire = gpd.read_file(os.path.join(\"data\", \"thomasfire.geojson\"))\n\n# Reproject Landsat data to match fire perimeter CRS (coordinate reference system)\nthomasfire.crs\nlandsat = landsat.rio.reproject(\"EPSG:3310\")\nassert landsat.rio.crs == thomasfire.crs\n\nLetâ€™s create a clipped version to use for plotting.\n\n# Create a false color image and clip it for plotting\nfalsecolor_thomasfire = landsat[[\"swir22\", \"nir08\", \"red\"]].to_array()\n\nfalsecolor_clip = falsecolor_thomasfire.rio.clip_box(*thomasfire.total_bounds)"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#plot-false-color-image-with-thomas-fire-perimeter",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#plot-false-color-image-with-thomas-fire-perimeter",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Plot False Color Image with Thomas Fire Perimeter",
    "text": "Plot False Color Image with Thomas Fire Perimeter\n\n# Create a map showing the false color image together with the Thomas Fire perimeter.\nfig, ax = plt.subplots(figsize=(10, 8))\n\nax.axis(\"off\")\n\nfalsecolor_thomasfire.plot.imshow(ax=ax, robust=True)\nthomasfire.geometry.boundary.plot(\n    ax=ax, color=\"darkred\", linewidth=1.5, label=\"Fire Perimeter\"\n)\n\nax.legend(fontsize=\"small\")\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.05)\n\nax.set_title(\"Thomas Fire (2017) Perimeter Map\")\n\nplt.figtext(0.01, 0.015, \"Data: NASA Landsat & CalFire\", ha=\"left\", fontsize=10)\n\nplt.show()\n\n\n\n\n\n\n\n\nLetâ€™s crop the map to see the fire scar more clearly.\n\n# Create a clipped map showing the false color image and the Thomas Fire perimeter\nfig, ax = plt.subplots(figsize=(10, 8))\n\nax.axis(\"off\")\n\nfalsecolor_clip.plot.imshow(ax=ax, robust=True)\nthomasfire.geometry.boundary.plot(\n    ax=ax, color=\"darkred\", linewidth=1.5, label=\"Fire Perimeter\"\n)\n\nax.legend(fontsize=\"small\")\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.05)\n\nax.set_title(\"Thomas Fire (2017) Perimeter - Clipped Map\")\n\nplt.figtext(0.01, 0.12, \"Data: NASA Landsat & CalFire\", ha=\"left\", fontsize=10)\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#analysis-of-false-color-maps",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#analysis-of-false-color-maps",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Analysis of False Color Maps",
    "text": "Analysis of False Color Maps\nThe above maps use false color satellite imagery to display the burn scar from the 2017 Thomas Fire and its perimeter. Shortwave infrared (SWIR) is plotted in red, near infrared (NIR) in green, and the red band in blue. Newly burned soil reflects SWIR light strongly, and plants containing water reflect NIR strongly, which makes this false color image helpful for visualizing fire effects."
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#compute-land-cover-statistics",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#compute-land-cover-statistics",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Compute land cover statistics",
    "text": "Compute land cover statistics\nNext, we will compute land cover statistics within the Thomas Fire perimeter in the following steps:\n\nUse the numpy function np.unique() to get the number of pixels per class in lulc_clip\n\n\nnp.unique(lulc, return_counts=True)[1]\n\narray([1927870,    6856,     361,  133704,    3638,   23150,    2907,\n          4974,    3727,   60203,    5102,   75925,     378,   11098,\n            29,      98,   64072,   69700,  183963,  454489,   88504,\n           799,     750,     684,      28,      90,     865,     262,\n          1329,      54,      44,      27,     298,      24,      15,\n            43,       7,   18996,    2181,     485,      13,      20,\n            10,    1858,   40653,    3782,     829,      58])\n\n\n\nCreate a data frame pix_counts with two columns: the code numbers for the pixels in lulc_clip and the number of pixels corresponding to each code\n\n\npix_counts = {\n    \"Class\": np.unique(lulc),\n    \"counts\": np.unique(lulc, return_counts=True)[1],\n}\n\npix_counts = pd.DataFrame(pix_counts)\npix_counts.head()\n\n\n\n\n\n\n\n\nClass\ncounts\n\n\n\n\n0\n0\n1927870\n\n\n1\n39\n6856\n\n\n2\n40\n361\n\n\n3\n41\n133704\n\n\n4\n42\n3638\n\n\n\n\n\n\n\n\nUse the labels data frame to add the class names to the codes in the pix_counts data frame. Store the resulting data frame as classes.\n\n\n# Rename the 'code' column in labels to match 'Class' in pix_counts\nlabels = labels.rename(columns={\"code\": \"Class\"})\n\n# Merge the dataframes, excluding rows where Class = 0\nclasses = pd.merge(pix_counts[pix_counts[\"Class\"] != 0], labels, on=\"Class\", how=\"left\")\nclasses.head()\n\n\n\n\n\n\n\n\nClass\ncounts\nclass_label\n\n\n\n\n0\n39\n6856\nCalifornia Central Valley Mixed Oak Savanna\n\n\n1\n40\n361\nCalifornia Coastal Closed-Cone Conifer Forest ...\n\n\n2\n41\n133704\nCalifornia Coastal Live Oak Woodland and Savanna\n\n\n3\n42\n3638\nCalifornia Lower Montane Blue Oak-Foothill Pin...\n\n\n4\n43\n23150\nCentral and Southern California Mixed Evergree...\n\n\n\n\n\n\n\n\nWhat area within the fire perimeter (in km^2) was estimated to be developed? The raster has a resolution of 30m.\n\n\n# Area of the Thomas Fire in square kilometers\nthomasfire_area_km = thomasfire.area / 1e6\nthomasfire_area_km\n\n0    1140.367254\ndtype: float64\n\n\n\n# Find the area of classes labeled as \"Developed\". These are located at 581-584.\narea_km_developed = (\n    classes[classes.Class.isin([581, 582, 583, 584])].counts.sum() * 30 * 30 / 1e6\n)\narea_km_developed\n\n40.7898\n\n\n\nWhat percent of the total area burned was developed?\n\n\nprint((area_km_developed / thomasfire_area_km * 100)[0], \"% \")\n\n3.5769003248981366 % \n\n\n\nStore the total number of pixels within the fire perimeter as a variable total_pixels\n\n\ntotal_pixels = classes.counts.sum()\ntotal_pixels\n\n1267082\n\n\n\nAdd the percentage of area covered by each class as a new column percentage to the classes data frame. Sort the data frame by percentage coverage in descending order.\n\n\nclasses[\"percent\"] = classes[\"counts\"] / classes[\"counts\"].sum() * 100\nclasses = classes.sort_values(by=\"percent\", ascending=False)\n\n\nCreate a horizontal bar plot showing the classes with more than 1% land cover in decreasing order.\n\n\n# Filter for classes with &gt;1% coverage and sort by percent in descending order\nfiltered_classes = classes[\n    (classes[\"percent\"] &gt; 1) & (classes[\"Class\"] != 0)\n].sort_values(\"percent\", ascending=True)\n\n# Create the horizontal bar plot\nplt.figure(figsize=(12, 8))\nplt.barh(\n    filtered_classes[\"class_label\"],\n    filtered_classes[\"percent\"],\n    height=0.5,\n    color=\"lightblue\",\n    edgecolor=\"black\",\n)\n\nplt.xlabel(\"Percent Land Cover\")\nplt.ylabel(\"Class Label\")\nplt.title(\n    \"Distribution of Land Cover within the Thomas Fire Perimeter, \\nLand Cover (&gt;1%)\",\n    fontsize=16,\n)\n\n# Adjust margins\nplt.margins(x=0.2)\n\nfor i, v in enumerate(filtered_classes[\"percent\"]):\n    plt.text(v, i, f\" {v:.1f}%\", va=\"center\")\n\nplt.xticks(rotation=0)\nplt.yticks(rotation=0)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#conclusion",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#conclusion",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "Conclusion",
    "text": "Conclusion\nHere we see the Thomas Fire predominantly affected coastal and chaparral ecosystems, with over 56% of the burned area consisting of coastal scrub and mixed chaparral vegetation types. Oak woodlands and savannas made up another significant portion, accounting for about 18% of the affected area. Only a small percentage of the burned area was developed land, indicating that the fire primarily affected natural ecosystems. Understanding the distribution of land cover types affected by the fire can help inform future land management decisions and restoration efforts in the area, and plan future fire management strategies. These findings highlight the importance of managing and protecting Californiaâ€™s coastal scrub and chaparral ecosystems, which are naturally adapted to fire but face increasing pressure from climate change and altered fire regimes."
  },
  {
    "objectID": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#references",
    "href": "posts/2024-12-04-thomas-fire-visualization/thomas-fire-blogpost.html#references",
    "title": "Thomas Fire: Perimeter Visualization & Land Cover Statistics",
    "section": "References",
    "text": "References\n\nFire Resource and Assessment Program. (2024). Historical Fire Perimeters. CalFire. https://www.fire.ca.gov/what-we-do/fire-resource-assessment-program/fire-perimeters. Accessed: November 23, 2024.\nNASA Landsat 8 (2024). Landsat Collection 2 Level-2 Science Products. Microsoft Planetary Computer. https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed: November 23, 2024.\nU.S. Geological Survey. (2016). GAP/LANDFIRE National Terrestrial Ecosystems Data 2011. USGS. https://www.sciencebase.gov/catalog/item/573cc51be4b0dae0d5e4b0c5. Accessed: November 23, 2024.\nGalaz-Garcia, Carmen. (2024). EDS 220 Assignment 4. UCSB MEDS. https://meds-eds-220.github.io/MEDS-eds-220-course/assignments/assignment4.html. Accessed: November 23, 2024."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Santa Barbaraâ€™s coastal waters host some of the most productive marine ecosystems on earth, Macrocystis pyrifera, or giant kelp forests. Kelp are a keystone species, and provide food, shelter and protection for all kinds of marine life, including commercially valuable fisheries. According to NOAA, factors influencing kelp forest growth include nutrients, light levels, temperatures, and ocean currents. Research by Smith et. al newly found that kelp use urea, in addition to ammonium, nitrate, dissolved organic nitrogen as nutrients for growth.\nIn this analysis, I would like to examine the relationship between ocean chemistry and kelp forest biomass, and develop a model for nutrient growth factors on biomass.\nThis exercise was produced as a part of the UCSB MEDS Program for EDS 222: Statistics for Environmental Data Science, taught by Dr.Â Max Czapanskiy.\n\n\n\n1. SBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000\n\nThese data are annual estimates of biomass of approximately 225 taxa of reef algae, invertebrates and fish in permanent transects at 11 kelp forest sites in the Santa Barbara Channel (2-8 transects per site). Abundance is measured annually (as percent cover or density, by size) and converted to biomass (i.e., wet mass, dry mass, decalcified dry mass, ash free dry mass) using published taxon-specific algorithms. Data collection began in summer 2000 and continues annually in summer to provide information on community structure, population dynamics and species change.\n\n\n\n\nSBC LTER Reef Sites\n\n\n2. SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000\n\nThese data contain water chemistry measurements taken monthly at these reefs in the nearshore areas of the Santa Barbra Channel, CA, USA: Arroyo Quemado, Bullito, Naples, Arroyo Burro, Mohawk and Carpinteria. Measurements include standard CTD parameters, nutrients, pigments, particulate CN, total dissolved N and P, stable isotopes of C and N (not all parameters are measured at all stations). Sampling began in November 2000. Starting in February 2003, a SBE19-Plus with a rosette sampler was used. Some stations are sampled only occasionally.\n\n\n\n\nSBC LTER Ocean Bottle Sites\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readtext)\nlibrary(janitor)\n\ntheme_set(theme_bw())\nset.seed(42)\n\n\n\n\nCode\n# Read in kelp and ocean chemistry data\nspecies &lt;- read_csv(here::here(\n  \"posts/2024-12-12-ocean-chem-kelp/data/Annual_All_Species_Biomass_at_transect_20240823.csv\"), na = \"-99999\") %&gt;%\n  janitor::clean_names()\n\nocean_chem &lt;- read.table(here::here(\"posts/2024-12-12-ocean-chem-kelp/data/LTER_monthly_bottledata_20240821.txt\"), \n  header = TRUE, sep = \";\", na = c(\"BDL\", \"99999\")) %&gt;% # `BDL` = below detection level\n  janitor::clean_names()\n\n# Filter to Giant kelp (Macrocystis pyrifera)\nkelp &lt;- species %&gt;%\n  filter(scientific_name == \"Macrocystis pyrifera\")\n\n\nThe variables I plan to use for ocean chemistry are (in units of micromoles per liter):\n\nno2_no3_umol_l = concentration of the sum of nitrate and nitrite\npon_umol_l = concentration of particulate organic nitrogen\ntdn_umol_l= concentration of dissolved nitrogen\nammonia_umol_l= concentration of ammonium\nurea_umol_l= concentration of urea\n\nThe variables I plan to use for kelp biomass are:\n\nwm_gm2 = areal wet mass of individuals or colony in grams per square meter\ndensity = density taxon per squared meter in number per square meter\n\nNote: there are other measures of biomass in ocean_chem. However, they are just linear transformations of wet mass.\n\n\n\n\n\nCode\n# Summarize kelp data to monthly averages\nmonthly_kelp_data &lt;- kelp %&gt;%\n  group_by(year, month, site) %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %&gt;%\n  mutate(date = as.Date(paste(year, month, \"01\", sep=\"-\")))\n\n# Summarize ocean chem data to monthly averages\nmonthly_ocean_data &lt;- ocean_chem %&gt;%\n  mutate(date = as.Date(yyyy_mm_dd),\n         year = lubridate::year(yyyy_mm_dd),\n         month = lubridate::month(yyyy_mm_dd)) %&gt;%\n  mutate(site = case_when(\n    station %in% c(\"QI\", \"QM\", \"QO\", \"QR\") ~ \"AQUE\", # match site names at both datasets\n    station %in% c(\"CI\", \"CO\", \"CR\") ~ \"CARP\",\n    station %in% c(\"MI\", \"MK\") ~ \"MOHK\",\n    station %in% c(\"NI\", \"NO\", \"NR\") ~ \"NAPL\",\n    station %in% c(\"BI\", \"BO\", \"BR\") ~ \"BULL\",\n    station %in% c(\"AB\") ~ \"ABUR\"\n  )) %&gt;%\n  group_by(year, month, site) %&gt;%\n  summarise(\n    across(where(is.numeric), mean),\n    n = dplyr::n()) %&gt;%\n  mutate(date = as.Date(paste(year, month, \"01\", sep=\"-\")))\n\n# Plot monthly average weight data\nggplot(monthly_kelp_data, aes(date, wm_gm2)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point() +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  labs(title = \"Monthly Average Giant Kelp Biomass by Site\",\n       x = \"Date\",\n       y = \"Wet Mass (g/mÂ²)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Check which sites have monthly ocean chemistry data\nggplot(monthly_ocean_data, aes(date, n)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Number of Ocean Chemistry Measurements by Site\",\n       x = \"Date\",\n       y = \"Number of Measurements\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nNote that there is a significant amount of missing data.\n\n\nCode\n# Join kelp and ocean chemistry datasets\njoined_data &lt;- left_join(\n  monthly_kelp_data, \n  monthly_ocean_data, \n  by = join_by(year, month, site, date)) %&gt;% \n  mutate(\n    site = as.factor(site),\n    pon_umol_l_log = log1p(pon_umol_l),\n    wm_gm2_log = log1p(wm_gm2))\n\n\nAfter grouping and summarizing the data, we are left with very few data points. There are NO instances that account for all of the ocean chemistry and kelp biomass variables initially identified for analysis.\nLooking at the joined data, the column with the most complete data from the original variables seems to be pon_umol_l.\n\n\n\n\n\nCode\n# Plot wet weight as a function of time\nggplot(joined_data, aes(date, wm_gm2)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Giant Kelp Wet Mass Over Time by Site\",\n       x = \"Date\", \n       y = \"Wet Mass (g/mÂ²)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen concentration as a function of time\nggplot(joined_data, aes(date, pon_umol_l)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Particulate Organic Nitrogen Concentration Over Time by Site\",\n       x = \"Date\",\n       y = \"PON Concentration (Î¼mol/L)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen as a function of wet mass\nggplot(joined_data, aes(wm_gm2, pon_umol_l)) +\n  geom_point(size = 1,\n             color = \"#6d8c23\") +\n  facet_wrap(~site) +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"PON Concentration vs Kelp Biomass by Site\",\n       x = \"Wet Mass (g/mÂ²)\",\n       y = \"PON Concentration (Î¼mol/L)\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen as a function of log wet mass\nggplot(joined_data, aes(wm_gm2_log, pon_umol_l)) +\n  geom_point(size = 1,\n             color = \"#6d8c23\") +\n  facet_wrap(~site) +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"PON Concentration vs Log Kelp Biomass by Site\",\n       x = \"Log Wet Mass (g/mÂ²)\",\n       y = \"PON Concentration (Î¼mol/L)\")\n\n\n\n\n\n\n\n\n\n\n\n\nIs there a relationship between biomass and particulate organic nitrogen (PON) concentration?\n\n\nCode\n# 1. Biomass and particulate organic nitrogen\nmodel_wm &lt;- lm(formula = wm_gm2 ~ pon_umol_l + site, \n               data = joined_data) \nmodel_wm %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = wm_gm2 ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3648.9 -1378.2  -418.3  1079.4  5162.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1246.88     673.76   1.851  0.06743 .  \npon_umol_l    147.54      53.95   2.735  0.00749 ** \nsiteNAPL      143.32     660.54   0.217  0.82870    \nsiteABUR    -1252.58     688.31  -1.820  0.07204 .  \nsiteAQUE      704.76     666.46   1.057  0.29307    \nsiteMOHK     3186.67     695.94   4.579 1.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2020 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.3592,    Adjusted R-squared:  0.3243 \nF-statistic: 10.31 on 5 and 92 DF,  p-value: 7.175e-08\n\n\nCode\n# 2. Log biomass and particulate organic nitrogen\nmodel_wm_log &lt;- lm(formula = wm_gm2_log ~ pon_umol_l + site, \n                   data = joined_data) \nmodel_wm_log %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = wm_gm2_log ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7046 -0.6219  0.3389  0.8050  3.1095 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.57754    0.48030  13.695  &lt; 2e-16 ***\npon_umol_l   0.06801    0.03846   1.768  0.08032 .  \nsiteNAPL     0.16902    0.47088   0.359  0.72046    \nsiteABUR    -1.38850    0.49067  -2.830  0.00572 ** \nsiteAQUE     0.45671    0.47509   0.961  0.33892    \nsiteMOHK     1.49788    0.49611   3.019  0.00328 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.44 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.2987,    Adjusted R-squared:  0.2605 \nF-statistic: 7.836 on 5 and 92 DF,  p-value: 3.512e-06\n\n\nCode\n# 3. Density and particulate organic nitrogen\nmodel_wm_density &lt;- lm(formula = density ~ pon_umol_l + site, \n                       data = joined_data) \nmodel_wm_density %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = density ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7742 -2.6087 -0.6179  1.8569 10.1785 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.7383     1.2152   1.430  0.15598    \npon_umol_l    0.3174     0.0973   3.261  0.00156 ** \nsiteNAPL      0.6812     1.1914   0.572  0.56885    \nsiteABUR     -1.8348     1.2414  -1.478  0.14285    \nsiteAQUE      1.4354     1.2020   1.194  0.23551    \nsiteMOHK      7.8533     1.2552   6.256 1.23e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.644 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.4621,    Adjusted R-squared:  0.4328 \nF-statistic: 15.81 on 5 and 92 DF,  p-value: 3.266e-11\n\n\n1. Biomass Model model_wm\nThere is a significant positive relationship between PON and kelp biomass (p = 0.00749). For every one Î¼mol/L increase in PON concentration, kelp wet mass increases by 147.54 grams/square meter when the reference site is BULL. Consistent with the â€œMonthly Average Giant Kelp Biomass by Siteâ€ plot, the MOHK site has significantly higher biomass than the other sites (coef = 3187.67, p = 1.46e-5). The model explains about 32.4% of the variation in biomass (not great).\n2. Log Biomass Model model_wm_log\nThere is a marginally significant positive relationship between PON and log-transformed kelp biomass (p = 0.080). For every one Î¼mol/L increase in PON concentration, log kelp wet mass increases by 0.068 units when the reference site is BULL. The ABUR site shows significantly lower biomass (coef = -1.389, p = 0.006) while MOHK shows significantly higher biomass (coef = 1.498, p = 0.003) compared to other sites. The model explains about 26.1% of the variation in log biomass (poor fit).\n3. Biomass Density Model model_wm_density\nThere is a significant positive relationship between PON and kelp density (p = 0.002). For every one Î¼mol/L increase in PON concentration, kelp density increases by 0.317 individuals per square meter when the reference site is BULL. Similar to the biomass models, the MOHK site shows significantly higher density than other sites (coef = 7.853, p &lt; 0.001). This model has the best fit of the three, explaining about 43.3% of the variation in density, but still is not great.\n\n\n\nHypotheses:\n\nH0: Particulate organic nitrogen has no effect on kelp biomass\nHA: Particulate organic nitrogen has an effect on kelp biomass\n\n\n\nCode\n# Get observed coefficient for pon_umol_l\nobserved_coef &lt;- coef(summary(model_wm))[\"pon_umol_l\", \"t value\"]\n\n# Create randomizations\nnull_dist &lt;- replicate(1000, {\n  rand_joined_data &lt;- joined_data %&gt;% \n    ungroup() %&gt;% \n    mutate(pon_umol_l = sample(pon_umol_l))\n  \n  rand_model &lt;- lm(wm_gm2 ~ pon_umol_l + site, data = rand_joined_data)\n  coef(summary(rand_model))[\"pon_umol_l\", \"t value\"]\n})\n\n# Calculate p-value (two-sided test)\np_value &lt;- mean(abs(null_dist) &gt;= abs(observed_coef))\n\n# Plot the null distribution\nnull_dist_df &lt;- data.frame(t_stat = null_dist)\nggplot(null_dist_df, aes(x = t_stat)) +\n  geom_histogram(binwidth = 0.2, \n                 fill = \"#6d8c23\", \n                 color = \"white\") +\n  geom_vline(xintercept = observed_coef, color = \"orchid\", linewidth = 1) +\n  labs(title = \"Null Distribution of PON Coefficient t-statistics\",\n       x = \"t-statistic\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Randomization test p-value:\", p_value, \"\\n\")\n\n\nRandomization test p-value: 0.003 \n\n\nCode\ncat(\"Observed t-statistic:\", observed_coef, \"\\n\")\n\n\nObserved t-statistic: 2.734869 \n\n\nThis randomization test shuffled the data n = 1000 times to determine what the point estimate (t-statistic) would be if it occurred by chance. 0nly 0.3% of the time would a value of t = 2.37 be produced as chance. At a threshold of alpha = 0.05, we can reject our null hypothesis and state that PON has a positive effect on kelp wet mass.\nThe observed t-statistic is 2.73, which means that the estimated coefficient is 2.73 standard errors away from zero. In the figure, the null distribution shows us what we would expect if there was no relationship between PON and biomass (measured by density). The purple line falls outside of the majority of the distribution, which tells us that there is a strong relationship between PON and density.\n\n\n\nDue to incomplete data, I could not perform my intended analysis. The ocean chemistry and kelp data were inconsistently collected across sites and months, leaving very few usable data points after cleaning.\nThe model likely suffers from omitted variable bias. While prior research has established that temperature, currents, and light levels affect kelp growth, my model only included PON and site as variables, excluding other nutrients and physical variables.\nAutocorrelation may have affected the results because samples were taken at monthly intervals, and ecological conditions typically carry over from one month to the next. This violation of independence could lead to underestimated standard errors and overstated significance in the statistical results. However, techniques to reduce autocorrelation, such as a temporal lag model, were not feasible given the substantial missing data, which included gaps of several months in the joined dataset.\nOverall, the results of my models should not be considered reliable due to the significant data gaps. I had overestimated the completeness of the LTER data for this research question.\nFor future research, I would like to analyze Thomas Fire effects on kelp growth using a more complete ocean chemistry dataset. Researchers at UCSB discovered that wildfire ash from the 2017 Thomas Fire resulted in significant additions of dissolved nutrients, including inorganic and organic nitrogen, silicic acid, metals, and organic carbon. Furthermore, this ash leachate resulted in an increase of relative abundance of eukaryotic phytoplankton. Since the impacts of wildfire products on Macrocystis pyrifera remain understudied, this could be a valuable area for investigation. Additional factors to consider would include kelp recruitment rates using a temporal lag model and dissolved metal concentrations.\n\n\n\n\n\n\nData\nCitation\nLink\n\n\n\n\nSBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000\nWashburn, L., M. Brzezinski, C. Carlson, and D. Siegel. 2024. SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000 ver 31. Environmental Data Initiative. https://doi.org/10.6073/pasta/cc75e947e0137e1594ebd8ce4b4a8880 (Accessed 2024-12-09).\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-sbc.10.31\n\n\nSBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000\nReed, D. and R. Miller. 2024. SBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000 ver 17. Environmental Data Initiative. https://doi.org/10.6073/pasta/6587ad06e299e566e2092d1268dc206b (Accessed 2024-12-10).\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-sbc.50.17"
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#background",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#background",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Santa Barbaraâ€™s coastal waters host some of the most productive marine ecosystems on earth, Macrocystis pyrifera, or giant kelp forests. Kelp are a keystone species, and provide food, shelter and protection for all kinds of marine life, including commercially valuable fisheries. According to NOAA, factors influencing kelp forest growth include nutrients, light levels, temperatures, and ocean currents. Research by Smith et. al newly found that kelp use urea, in addition to ammonium, nitrate, dissolved organic nitrogen as nutrients for growth.\nIn this analysis, I would like to examine the relationship between ocean chemistry and kelp forest biomass, and develop a model for nutrient growth factors on biomass.\nThis exercise was produced as a part of the UCSB MEDS Program for EDS 222: Statistics for Environmental Data Science, taught by Dr.Â Max Czapanskiy."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#dataset-descriptions",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#dataset-descriptions",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "1. SBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000\n\nThese data are annual estimates of biomass of approximately 225 taxa of reef algae, invertebrates and fish in permanent transects at 11 kelp forest sites in the Santa Barbara Channel (2-8 transects per site). Abundance is measured annually (as percent cover or density, by size) and converted to biomass (i.e., wet mass, dry mass, decalcified dry mass, ash free dry mass) using published taxon-specific algorithms. Data collection began in summer 2000 and continues annually in summer to provide information on community structure, population dynamics and species change.\n\n\n\n\nSBC LTER Reef Sites\n\n\n2. SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000\n\nThese data contain water chemistry measurements taken monthly at these reefs in the nearshore areas of the Santa Barbra Channel, CA, USA: Arroyo Quemado, Bullito, Naples, Arroyo Burro, Mohawk and Carpinteria. Measurements include standard CTD parameters, nutrients, pigments, particulate CN, total dissolved N and P, stable isotopes of C and N (not all parameters are measured at all stations). Sampling began in November 2000. Starting in February 2003, a SBE19-Plus with a rosette sampler was used. Some stations are sampled only occasionally.\n\n\n\n\nSBC LTER Ocean Bottle Sites\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readtext)\nlibrary(janitor)\n\ntheme_set(theme_bw())\nset.seed(42)\n\n\n\n\nCode\n# Read in kelp and ocean chemistry data\nspecies &lt;- read_csv(here::here(\n  \"posts/2024-12-12-ocean-chem-kelp/data/Annual_All_Species_Biomass_at_transect_20240823.csv\"), na = \"-99999\") %&gt;%\n  janitor::clean_names()\n\nocean_chem &lt;- read.table(here::here(\"posts/2024-12-12-ocean-chem-kelp/data/LTER_monthly_bottledata_20240821.txt\"), \n  header = TRUE, sep = \";\", na = c(\"BDL\", \"99999\")) %&gt;% # `BDL` = below detection level\n  janitor::clean_names()\n\n# Filter to Giant kelp (Macrocystis pyrifera)\nkelp &lt;- species %&gt;%\n  filter(scientific_name == \"Macrocystis pyrifera\")\n\n\nThe variables I plan to use for ocean chemistry are (in units of micromoles per liter):\n\nno2_no3_umol_l = concentration of the sum of nitrate and nitrite\npon_umol_l = concentration of particulate organic nitrogen\ntdn_umol_l= concentration of dissolved nitrogen\nammonia_umol_l= concentration of ammonium\nurea_umol_l= concentration of urea\n\nThe variables I plan to use for kelp biomass are:\n\nwm_gm2 = areal wet mass of individuals or colony in grams per square meter\ndensity = density taxon per squared meter in number per square meter\n\nNote: there are other measures of biomass in ocean_chem. However, they are just linear transformations of wet mass."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#clean-data",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#clean-data",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Code\n# Summarize kelp data to monthly averages\nmonthly_kelp_data &lt;- kelp %&gt;%\n  group_by(year, month, site) %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE)) %&gt;%\n  mutate(date = as.Date(paste(year, month, \"01\", sep=\"-\")))\n\n# Summarize ocean chem data to monthly averages\nmonthly_ocean_data &lt;- ocean_chem %&gt;%\n  mutate(date = as.Date(yyyy_mm_dd),\n         year = lubridate::year(yyyy_mm_dd),\n         month = lubridate::month(yyyy_mm_dd)) %&gt;%\n  mutate(site = case_when(\n    station %in% c(\"QI\", \"QM\", \"QO\", \"QR\") ~ \"AQUE\", # match site names at both datasets\n    station %in% c(\"CI\", \"CO\", \"CR\") ~ \"CARP\",\n    station %in% c(\"MI\", \"MK\") ~ \"MOHK\",\n    station %in% c(\"NI\", \"NO\", \"NR\") ~ \"NAPL\",\n    station %in% c(\"BI\", \"BO\", \"BR\") ~ \"BULL\",\n    station %in% c(\"AB\") ~ \"ABUR\"\n  )) %&gt;%\n  group_by(year, month, site) %&gt;%\n  summarise(\n    across(where(is.numeric), mean),\n    n = dplyr::n()) %&gt;%\n  mutate(date = as.Date(paste(year, month, \"01\", sep=\"-\")))\n\n# Plot monthly average weight data\nggplot(monthly_kelp_data, aes(date, wm_gm2)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point() +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  labs(title = \"Monthly Average Giant Kelp Biomass by Site\",\n       x = \"Date\",\n       y = \"Wet Mass (g/mÂ²)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Check which sites have monthly ocean chemistry data\nggplot(monthly_ocean_data, aes(date, n)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Number of Ocean Chemistry Measurements by Site\",\n       x = \"Date\",\n       y = \"Number of Measurements\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nNote that there is a significant amount of missing data.\n\n\nCode\n# Join kelp and ocean chemistry datasets\njoined_data &lt;- left_join(\n  monthly_kelp_data, \n  monthly_ocean_data, \n  by = join_by(year, month, site, date)) %&gt;% \n  mutate(\n    site = as.factor(site),\n    pon_umol_l_log = log1p(pon_umol_l),\n    wm_gm2_log = log1p(wm_gm2))\n\n\nAfter grouping and summarizing the data, we are left with very few data points. There are NO instances that account for all of the ocean chemistry and kelp biomass variables initially identified for analysis.\nLooking at the joined data, the column with the most complete data from the original variables seems to be pon_umol_l."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#visualize-data",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#visualize-data",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Code\n# Plot wet weight as a function of time\nggplot(joined_data, aes(date, wm_gm2)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Giant Kelp Wet Mass Over Time by Site\",\n       x = \"Date\", \n       y = \"Wet Mass (g/mÂ²)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen concentration as a function of time\nggplot(joined_data, aes(date, pon_umol_l)) +\n  geom_line(color = \"#6d8c23\") +\n  geom_point(size = 0.1) +\n  facet_wrap(~site) +\n  scale_x_date(date_breaks = \"4 years\",\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"Particulate Organic Nitrogen Concentration Over Time by Site\",\n       x = \"Date\",\n       y = \"PON Concentration (Î¼mol/L)\") +\n  theme(axis.text.x = element_text(size = 6), angle = 45, hjust = 1)\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen as a function of wet mass\nggplot(joined_data, aes(wm_gm2, pon_umol_l)) +\n  geom_point(size = 1,\n             color = \"#6d8c23\") +\n  facet_wrap(~site) +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"PON Concentration vs Kelp Biomass by Site\",\n       x = \"Wet Mass (g/mÂ²)\",\n       y = \"PON Concentration (Î¼mol/L)\")\n\n\n\n\n\n\n\n\n\nCode\n# Plot particulate organic nitrogen as a function of log wet mass\nggplot(joined_data, aes(wm_gm2_log, pon_umol_l)) +\n  geom_point(size = 1,\n             color = \"#6d8c23\") +\n  facet_wrap(~site) +\n  scale_y_continuous(expand = expansion(c(0, 0))) +\n  labs(title = \"PON Concentration vs Log Kelp Biomass by Site\",\n       x = \"Log Wet Mass (g/mÂ²)\",\n       y = \"PON Concentration (Î¼mol/L)\")"
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#run-models",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#run-models",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Is there a relationship between biomass and particulate organic nitrogen (PON) concentration?\n\n\nCode\n# 1. Biomass and particulate organic nitrogen\nmodel_wm &lt;- lm(formula = wm_gm2 ~ pon_umol_l + site, \n               data = joined_data) \nmodel_wm %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = wm_gm2 ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3648.9 -1378.2  -418.3  1079.4  5162.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1246.88     673.76   1.851  0.06743 .  \npon_umol_l    147.54      53.95   2.735  0.00749 ** \nsiteNAPL      143.32     660.54   0.217  0.82870    \nsiteABUR    -1252.58     688.31  -1.820  0.07204 .  \nsiteAQUE      704.76     666.46   1.057  0.29307    \nsiteMOHK     3186.67     695.94   4.579 1.46e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2020 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.3592,    Adjusted R-squared:  0.3243 \nF-statistic: 10.31 on 5 and 92 DF,  p-value: 7.175e-08\n\n\nCode\n# 2. Log biomass and particulate organic nitrogen\nmodel_wm_log &lt;- lm(formula = wm_gm2_log ~ pon_umol_l + site, \n                   data = joined_data) \nmodel_wm_log %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = wm_gm2_log ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7046 -0.6219  0.3389  0.8050  3.1095 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.57754    0.48030  13.695  &lt; 2e-16 ***\npon_umol_l   0.06801    0.03846   1.768  0.08032 .  \nsiteNAPL     0.16902    0.47088   0.359  0.72046    \nsiteABUR    -1.38850    0.49067  -2.830  0.00572 ** \nsiteAQUE     0.45671    0.47509   0.961  0.33892    \nsiteMOHK     1.49788    0.49611   3.019  0.00328 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.44 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.2987,    Adjusted R-squared:  0.2605 \nF-statistic: 7.836 on 5 and 92 DF,  p-value: 3.512e-06\n\n\nCode\n# 3. Density and particulate organic nitrogen\nmodel_wm_density &lt;- lm(formula = density ~ pon_umol_l + site, \n                       data = joined_data) \nmodel_wm_density %&gt;% \n  summary()\n\n\n\nCall:\nlm(formula = density ~ pon_umol_l + site, data = joined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7742 -2.6087 -0.6179  1.8569 10.1785 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.7383     1.2152   1.430  0.15598    \npon_umol_l    0.3174     0.0973   3.261  0.00156 ** \nsiteNAPL      0.6812     1.1914   0.572  0.56885    \nsiteABUR     -1.8348     1.2414  -1.478  0.14285    \nsiteAQUE      1.4354     1.2020   1.194  0.23551    \nsiteMOHK      7.8533     1.2552   6.256 1.23e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.644 on 92 degrees of freedom\n  (166 observations deleted due to missingness)\nMultiple R-squared:  0.4621,    Adjusted R-squared:  0.4328 \nF-statistic: 15.81 on 5 and 92 DF,  p-value: 3.266e-11\n\n\n1. Biomass Model model_wm\nThere is a significant positive relationship between PON and kelp biomass (p = 0.00749). For every one Î¼mol/L increase in PON concentration, kelp wet mass increases by 147.54 grams/square meter when the reference site is BULL. Consistent with the â€œMonthly Average Giant Kelp Biomass by Siteâ€ plot, the MOHK site has significantly higher biomass than the other sites (coef = 3187.67, p = 1.46e-5). The model explains about 32.4% of the variation in biomass (not great).\n2. Log Biomass Model model_wm_log\nThere is a marginally significant positive relationship between PON and log-transformed kelp biomass (p = 0.080). For every one Î¼mol/L increase in PON concentration, log kelp wet mass increases by 0.068 units when the reference site is BULL. The ABUR site shows significantly lower biomass (coef = -1.389, p = 0.006) while MOHK shows significantly higher biomass (coef = 1.498, p = 0.003) compared to other sites. The model explains about 26.1% of the variation in log biomass (poor fit).\n3. Biomass Density Model model_wm_density\nThere is a significant positive relationship between PON and kelp density (p = 0.002). For every one Î¼mol/L increase in PON concentration, kelp density increases by 0.317 individuals per square meter when the reference site is BULL. Similar to the biomass models, the MOHK site shows significantly higher density than other sites (coef = 7.853, p &lt; 0.001). This model has the best fit of the three, explaining about 43.3% of the variation in density, but still is not great."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#randomization-test",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#randomization-test",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Hypotheses:\n\nH0: Particulate organic nitrogen has no effect on kelp biomass\nHA: Particulate organic nitrogen has an effect on kelp biomass\n\n\n\nCode\n# Get observed coefficient for pon_umol_l\nobserved_coef &lt;- coef(summary(model_wm))[\"pon_umol_l\", \"t value\"]\n\n# Create randomizations\nnull_dist &lt;- replicate(1000, {\n  rand_joined_data &lt;- joined_data %&gt;% \n    ungroup() %&gt;% \n    mutate(pon_umol_l = sample(pon_umol_l))\n  \n  rand_model &lt;- lm(wm_gm2 ~ pon_umol_l + site, data = rand_joined_data)\n  coef(summary(rand_model))[\"pon_umol_l\", \"t value\"]\n})\n\n# Calculate p-value (two-sided test)\np_value &lt;- mean(abs(null_dist) &gt;= abs(observed_coef))\n\n# Plot the null distribution\nnull_dist_df &lt;- data.frame(t_stat = null_dist)\nggplot(null_dist_df, aes(x = t_stat)) +\n  geom_histogram(binwidth = 0.2, \n                 fill = \"#6d8c23\", \n                 color = \"white\") +\n  geom_vline(xintercept = observed_coef, color = \"orchid\", linewidth = 1) +\n  labs(title = \"Null Distribution of PON Coefficient t-statistics\",\n       x = \"t-statistic\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Randomization test p-value:\", p_value, \"\\n\")\n\n\nRandomization test p-value: 0.003 \n\n\nCode\ncat(\"Observed t-statistic:\", observed_coef, \"\\n\")\n\n\nObserved t-statistic: 2.734869 \n\n\nThis randomization test shuffled the data n = 1000 times to determine what the point estimate (t-statistic) would be if it occurred by chance. 0nly 0.3% of the time would a value of t = 2.37 be produced as chance. At a threshold of alpha = 0.05, we can reject our null hypothesis and state that PON has a positive effect on kelp wet mass.\nThe observed t-statistic is 2.73, which means that the estimated coefficient is 2.73 standard errors away from zero. In the figure, the null distribution shows us what we would expect if there was no relationship between PON and biomass (measured by density). The purple line falls outside of the majority of the distribution, which tells us that there is a strong relationship between PON and density."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#conclusion",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#conclusion",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Due to incomplete data, I could not perform my intended analysis. The ocean chemistry and kelp data were inconsistently collected across sites and months, leaving very few usable data points after cleaning.\nThe model likely suffers from omitted variable bias. While prior research has established that temperature, currents, and light levels affect kelp growth, my model only included PON and site as variables, excluding other nutrients and physical variables.\nAutocorrelation may have affected the results because samples were taken at monthly intervals, and ecological conditions typically carry over from one month to the next. This violation of independence could lead to underestimated standard errors and overstated significance in the statistical results. However, techniques to reduce autocorrelation, such as a temporal lag model, were not feasible given the substantial missing data, which included gaps of several months in the joined dataset.\nOverall, the results of my models should not be considered reliable due to the significant data gaps. I had overestimated the completeness of the LTER data for this research question.\nFor future research, I would like to analyze Thomas Fire effects on kelp growth using a more complete ocean chemistry dataset. Researchers at UCSB discovered that wildfire ash from the 2017 Thomas Fire resulted in significant additions of dissolved nutrients, including inorganic and organic nitrogen, silicic acid, metals, and organic carbon. Furthermore, this ash leachate resulted in an increase of relative abundance of eukaryotic phytoplankton. Since the impacts of wildfire products on Macrocystis pyrifera remain understudied, this could be a valuable area for investigation. Additional factors to consider would include kelp recruitment rates using a temporal lag model and dissolved metal concentrations."
  },
  {
    "objectID": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#data-citations",
    "href": "posts/2024-12-12-ocean-chem-kelp/final-project-clean.html#data-citations",
    "title": "Ocean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations",
    "section": "",
    "text": "Data\nCitation\nLink\n\n\n\n\nSBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000\nWashburn, L., M. Brzezinski, C. Carlson, and D. Siegel. 2024. SBC LTER: Ocean: Ocean Currents and Biogeochemistry: Nearshore water profiles (monthly CTD and chemistry), ongoing since 2000 ver 31. Environmental Data Initiative. https://doi.org/10.6073/pasta/cc75e947e0137e1594ebd8ce4b4a8880 (Accessed 2024-12-09).\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-sbc.10.31\n\n\nSBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000\nReed, D. and R. Miller. 2024. SBC LTER: Reef: Annual time series of biomass for kelp forest species, ongoing since 2000 ver 17. Environmental Data Initiative. https://doi.org/10.6073/pasta/6587ad06e299e566e2092d1268dc206b (Accessed 2024-12-10).\nhttps://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-sbc.50.17"
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html",
    "title": "Voter Registration Data Infographic",
    "section": "",
    "text": "Voter Registration Data Infographic"
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#why-voter-data",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#why-voter-data",
    "title": "Voter Registration Data Infographic",
    "section": "Why voter data?",
    "text": "Why voter data?\nSince childhood, Iâ€™ve been interested in politicsâ€“ a fault I attribute to my dad. We were your typical liberal Prius-driving NPR listeners and CNN-blaring-in-the-background household. Upon moving to Santa Barbara in 2017 for my undergraduate degree, Iâ€™ve volunteered or worked in every election, traveling up and down the county to knock on doors for local candidates with pro-environment and pro-housing values.\n\n\n\nSome campaigns over the years\n\n\nMost visualizations of voter data happen at the national or state level, rather than within counties. Furthermore, election data from the 2024 general election has not been fully certified yet for most counties in California. I aim to provide a quick overview of the political composition of our county using voter registration and census data for people interested in local politics. A few questions Iâ€™d like to answer include:\n\nHow are Democratic and Republican voters spatially distributed throughout the county?\nWhat is the political composition of the county?\nIs there a partisan difference between voter ages?\nHow do voter registrations change over time? Do they increase in frequency closer to elections?"
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#about-the-data",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#about-the-data",
    "title": "Voter Registration Data Infographic",
    "section": "About the data",
    "text": "About the data\nThe Santa Barbara County voter registration data used for this project was requested from SB County Elections, and is dated for May 9, 2025. It contains voter registration information (voter name, address, contact information, voting precinct, and political party affiliation) for all voters in the county.\nFor census data, I use the tidycensus and tigris packages, with 2020 and 2023 geometries and population surveys, and tidygeocoder for addresses."
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#visualizations",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#visualizations",
    "title": "Voter Registration Data Infographic",
    "section": "Visualizations",
    "text": "Visualizations\n\nHow are voters distributed across the county? I wanted to plot the difference in percentage of Democrats compared to Republicans across the county. I chose a chloropleth map to visualize this, because it is a good way to display spatial relationships. In Santa Barbara, urban centers in South County tend to lean heavily Democratic, while more rural areas are Republican. One challenge with displaying partisanship on a chloropleth map, however, is the visual overrepresentation of sparsely populated but large geographic areas. It appears as though a large portion of the county is red, when in reality very few people live in those areas, and the county as a whole leans solidly Democratic. Census blocks tend to be smaller than tracts, and areas with no population were excluded, therefore lessening the visual effect of area.\nWhat is the partisan breakdown of the county? Iâ€™ve included a waffle plot displaying the proportion of voters by party to visualize the percentage of voters belonging to each political party.\nHow old are Democrats vs.Â Republicans? For this visualization, I chose stacked bar charts to visualize the age breakdowns within each party and across the county. On average, Democrats are slightly younger than Republicans.\nWhen do people register to vote? Voters generally register to vote closer to elections, and we can see that there is a noticeable spike in the line graph of number of registrations each general election year (labeled on the x-axis). When we look at the weekly and daily registration trends leading up to the 2024 general election, we see a spike in registrations around the registration deadline and on election day."
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#code-for-visualizations",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#code-for-visualizations",
    "title": "Voter Registration Data Infographic",
    "section": "Code for Visualizations",
    "text": "Code for Visualizations\n\n\nShow code\n# -------------- Setup --------------\n# Load packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(here)\nlibrary(tidycensus)\nlibrary(ggmap)\nlibrary(sf)\nlibrary(tigris)\nlibrary(showtext)\nlibrary(sysfonts)\nlibrary(tidygeocoder)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(waffle)\nlibrary(patchwork)\nlibrary(leaflet)\n\n# Set census API key\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n\n\n\nShow code\n# -------------- Customize Theme --------------\n# Customize font and theme\nfont_add_google(\"EB Garamond\", \"EBGaramond\")\nshowtext_auto()\ntheme_set(theme_bw())\n\n# Custom colors\nparty_colors &lt;- c(\n    \"DEM\" = \"#126ca8\",\n    \"REP\" = \"#c83236\",\n    \"NPP\" = \"#c9b1d4\",\n    \"Other\" = \"gray80\"\n)\n\n# Create custom function\ntheme_voter_viz &lt;- function(base_size = 20, base_family = \"EBGaramond\") {\n    theme_classic() %+replace%\n        theme(\n            panel.grid.major.y = element_blank(),\n            text = element_text(size = base_size, family = base_family),\n            axis.text = element_text(family = base_family),\n            plot.title = element_text(face = \"bold\", family = base_family),\n            plot.subtitle = element_text(family = base_family),\n            plot.caption = element_text(family = base_family),\n            legend.text = element_text(family = base_family)\n        )\n}\n\n\n\n\nShow code\n# -------------- Load data --------------\nfull_file &lt;- read_delim(\n    here::here(\"posts/2025-03-17-voter-data-viz/data/Countywide_Voter Registration 030325.TXT\"),\n    delim = \"\\t\"\n) %&gt;%\n    clean_names()\n\n\n\nVisualization 1: Spatial Analysis of Voter Distribution\n\n\nShow code\n# -------------- Geocoding & Retrieval of Census Data --------------\n# Prepare addresses for geocoding\n# address_full &lt;- full_file %&gt;%\n#     mutate(street_address_test = paste(address_number, street_name, sep = \" \"))\n#\n# # Geocoding function\n# geocode_addresses &lt;- function() {\n#     census_list &lt;- list()\n#     for (j in 1:25) {\n#         start_index &lt;- ((j - 1) * 10000) + 1\n#         end_index &lt;- j * 10000\n#\n#         census_list[[j]] &lt;- address_full[start_index:end_index, ] %&gt;%\n#             tidygeocoder::geocode(\n#                 street = street_address_test,\n#                 city = city,\n#                 state = state,\n#                 method = \"census\"\n#             )\n#     }\n#\n#     final_census_df &lt;- bind_rows(census_list) %&gt;%\n#         filter(party_code %in% c(\"DEM\", \"REP\", \"NPP\")) %&gt;%\n#         drop_na(lat)\n#\n#     # Save for future use\n#     saveRDS(final_census_df, \"geocoded_addresses.rds\")\n#\n#     return(final_census_df)\n# }\n#\n# final_census_df &lt;- geocode_addresses()\n\nfinal_census_df &lt;- readRDS(here::here(\"posts/2025-03-17-voter-data-viz/data/geocoded_addresses.rds\"))\n\n# File paths for saved data\nsb_pop_path &lt;- here::here(\"posts/2025-03-17-voter-data-viz/data/sb_pop_2023.rds\")\nsb_blocks_path &lt;- here::here(\"posts/2025-03-17-voter-data-viz/data/sb_blocks_2020.rds\")\n\n# Check if files exist, otherwise create and save them\nif (!file.exists(sb_pop_path)) {\n    # Get Santa Barbara census tract population data\n    sb_pop_2023 &lt;- get_acs(\n        geography = \"tract\",\n        variables = \"B01003_001\", # Total population variable\n        state = \"CA\",\n        county = \"Santa Barbara\",\n        year = 2023,\n        geometry = TRUE\n    ) %&gt;%\n        st_transform(crs = 4326) %&gt;%\n        filter(GEOID != 06083990000) # Channel Islands tract\n\n    # Save to RDS file\n    saveRDS(sb_pop_2023, sb_pop_path)\n} else {\n    # Load from saved file\n    sb_pop_2023 &lt;- readRDS(sb_pop_path)\n}\n\nif (!file.exists(sb_blocks_path)) {\n    # Get census block geography for Santa Barbara County\n    sb_blocks_2020 &lt;- get_decennial(\n        geography = \"block\",\n        variables = \"P1_001N\", # Total population variable\n        state = \"CA\",\n        county = \"Santa Barbara\",\n        year = 2020,\n        geometry = TRUE\n    ) %&gt;%\n        st_transform(crs = 4326) %&gt;%\n        # filter out ocean tracts\n        filter(!grepl(\"^06083990000\", GEOID))\n\n    # Save to RDS file\n    saveRDS(sb_blocks_2020, sb_blocks_path)\n} else {\n    # Load from saved file\n    sb_blocks_2020 &lt;- readRDS(sb_blocks_path)\n}\n\n\n\n\nShow code\n# Tract-level map code here\n# -------------- Spatial Visualization: Census Tracts --------------\n# Convert geocoded addresses to sf object\nfinal_census_sf &lt;- st_as_sf(\n    final_census_df,\n    coords = c(\"long\", \"lat\"),\n    crs = 4326\n)\n\n# Join voters to tracts and calculate party percentages\nparty_by_tract &lt;- st_join(final_census_sf, sb_pop_2023) %&gt;%\n    st_drop_geometry() %&gt;%\n    count(GEOID, party_code) %&gt;%\n    group_by(GEOID) %&gt;%\n    mutate(tract_total = sum(n), percentage = n / tract_total * 100) %&gt;%\n    ungroup()\n\n# Two-party comparison calculation\ntwo_party_comparison &lt;- party_by_tract %&gt;%\n    filter(party_code %in% c(\"DEM\", \"REP\")) %&gt;%\n    group_by(GEOID) %&gt;%\n    mutate(two_party_total = sum(n)) %&gt;%\n    mutate(two_party_pct = n / two_party_total * 100) %&gt;%\n    select(GEOID, party_code, two_party_pct) %&gt;%\n    pivot_wider(names_from = party_code, values_from = two_party_pct) %&gt;%\n    mutate(diff = DEM - REP)\n\n# Join and create map\ntract_diff_map &lt;- sb_pop_2023 %&gt;%\n    select(GEOID, geometry) %&gt;%\n    left_join(two_party_comparison, by = \"GEOID\") %&gt;%\n    filter(!is.na(diff))\n\n# Create custom breaks and labels for the scale\nbreaks &lt;- seq(-100, 100, by = 20)\nlabels &lt;- sapply(breaks, function(x) {\n    if (x &gt; 0) {\n        paste0(\"+\", x)\n    } else {\n        x\n    }\n})\n\n# Create the divergent map\nggplot() +\n    geom_sf(data = tract_diff_map, aes(fill = diff)) +\n    scale_fill_gradient2(\n        low = party_colors[\"REP\"],\n        mid = \"#FFFFFF\",\n        high = party_colors[\"DEM\"],\n        midpoint = 0,\n        name = \"Dem-Rep Difference (%)\",\n        breaks = breaks,\n        labels = labels,\n        guide = guide_colorbar(\n            direction = \"horizontal\",\n            barwidth = 15,\n            barheight = 0.5,\n            title.position = \"top\",\n            title.hjust = 0.5\n        )\n    ) +\n    labs(\n        title = \"Democratic vs. Republican Voter Distribution by Census Tract\\n\",\n        subtitle = \"Percentage difference between parties (excluding other affiliations)\"\n    ) +\n    theme_void() +\n    theme_voter_viz() +\n    theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        legend.margin = margin(t = 10, b = 10),\n        axis.title = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"tract_diff_map.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 13\n)\n\n# Get bounding box\ntract_map_bbox &lt;- st_bbox(tract_diff_map)\n\n\n\n\nShow code\n# Block-level map code here\n# -------------- Spatial Visualization: Block Groups --------------\n# Join voters to blocks and calculate party percentages\nparty_by_block &lt;- st_join(final_census_sf, sb_blocks_2020) %&gt;%\n    st_drop_geometry() %&gt;%\n    count(GEOID, party_code) %&gt;%\n    group_by(GEOID) %&gt;%\n    mutate(block_total = sum(n), percentage = n / block_total * 100) %&gt;%\n    ungroup()\n\n# Two-party comparison calculation\ntwo_party_comparison &lt;- party_by_block %&gt;%\n    filter(party_code %in% c(\"DEM\", \"REP\")) %&gt;%\n    group_by(GEOID) %&gt;%\n    mutate(\n        two_party_total = sum(n),\n        two_party_pct = n / two_party_total * 100\n    ) %&gt;%\n    select(GEOID, party_code, two_party_pct) %&gt;%\n    pivot_wider(names_from = party_code, values_from = two_party_pct) %&gt;%\n    mutate(diff = DEM - REP)\n\n# Join and create map\nblock_diff_map &lt;- sb_blocks_2020 %&gt;%\n    select(GEOID, geometry) %&gt;%\n    left_join(two_party_comparison, by = \"GEOID\")\n\n# Create custom breaks and labels for the scale\nbreaks &lt;- seq(-100, 100, by = 20)\nlabels &lt;- sapply(breaks, function(x) {\n    if (x &gt; 0) {\n        paste0(\"+\", x)\n    } else {\n        x\n    }\n})\n\n# Create divergent plot\nggplot() +\n    geom_sf(data = block_diff_map, aes(fill = diff), color = NA, size = 0.5) +\n    geom_sf(data = filter(block_diff_map, is.na(diff)), fill = \"gray80\", color = \"gray30\") +\n    scale_fill_gradient2(\n        low = party_colors[\"REP\"],\n        mid = \"#FFFFFF\",\n        high = party_colors[\"DEM\"],\n        midpoint = 0,\n        name = \"Dem-Rep Difference (%)\",\n        breaks = breaks,\n        labels = labels,\n        guide = guide_colorbar(\n            direction = \"horizontal\",\n            barwidth = 15,\n            barheight = 0.5,\n            title.position = \"top\",\n            title.hjust = 0.5\n        ),\n        na.value = \"gray80\"\n    ) +\n    coord_sf(\n        xlim = c(tract_map_bbox[\"xmin\"], tract_map_bbox[\"xmax\"]),\n        ylim = c(tract_map_bbox[\"ymin\"], tract_map_bbox[\"ymax\"])\n    ) +\n    labs(\n        title = \"Democratic vs. Republican Voter Distribution by Census Block\\n\",\n        subtitle = \"Percentage difference between parties (excluding other affiliations)\"\n    ) +\n    theme_void() +\n    theme_voter_viz() +\n    theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        legend.margin = margin(t = 10, b = 10),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"block_diff_map.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 13\n)\n\n\n\n\nShow code\n# Create a bounding box for a zoomed-in look at south county\nsouth_county_bbox &lt;- tigris::places(state = \"CA\", cb = TRUE) %&gt;%\n    filter(\n        NAME %in%\n            c(\n                \"Santa Barbara\",\n                \"Goleta\",\n                \"Carpinteria\",\n                \"Isla Vista\",\n                \"Summerland\",\n                \"Montecito\",\n                \"Eastern Goleta Valley\",\n                \"Mission Canyon\",\n                \"University of California-Santa Barbara\",\n                \"Toro Canyon\"\n            )\n    ) %&gt;%\n    st_union() %&gt;%\n    st_buffer(dist = 0.02)\n\n\n\n\nShow code\nggplot() +\n    geom_sf(data = block_diff_map, aes(fill = diff), color = NA, size = 0.5) +\n    geom_sf(data = filter(block_diff_map, is.na(diff)), fill = \"gray80\", color = \"gray30\") +\n    scale_fill_gradient2(\n        low = party_colors[\"REP\"],\n        mid = \"#FFFFFF\",\n        high = party_colors[\"DEM\"],\n        midpoint = 0,\n        name = \"Dem-Rep Difference (%)\",\n        breaks = breaks,\n        labels = labels,\n        guide = guide_colorbar(\n            direction = \"horizontal\",\n            barwidth = 15,\n            barheight = 0.5,\n            title.position = \"top\",\n            title.hjust = 0.5\n        ),\n        na.value = \"gray80\"\n    ) +\n    coord_sf(\n        xlim = st_bbox(south_county_bbox)[c(1, 3)],\n        ylim = st_bbox(south_county_bbox)[c(2, 4)]\n    ) +\n    theme_void() +\n    theme_voter_viz() +\n    theme(\n        legend.position = \"bottom\",\n        legend.title = element_text(size = 10),\n        legend.text = element_text(size = 8),\n        legend.margin = margin(t = 10, b = 10),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank()\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"zoom_block_diff_map.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 10\n)\n\n\n\n\nShow code\n# -------------- Spatial Visualization: Interactive Census Block Groups --------------\n# Create a diverging color palette for leaflet map\npal &lt;- colorNumeric(\n    palette = colorRampPalette(c(\n        party_colors[\"REP\"],\n        \"#FFFFFF\",\n        party_colors[\"DEM\"]\n    ))(100),\n    domain = c(-100, 100), # Set fixed domain from -100 to +100\n    na.color = \"transparent\"\n)\n\n# Create leaflet map with OpenStreetMap tiles\nleaflet(block_diff_map) %&gt;%\n    addTiles() %&gt;%\n    addPolygons(\n        fillColor = ~ pal(diff),\n        weight = 1,\n        opacity = 1,\n        color = \"white\",\n        dashArray = \"3\",\n        fillOpacity = 0.7,\n        label = ~ paste0(\"Dem-Rep Difference: \", round(diff, 1), \"%\")\n    ) %&gt;%\n    addLegend(\n        pal = pal,\n        values = ~diff,\n        opacity = 0.7,\n        title = \"Dem-Rep Difference (%)\",\n        position = \"bottomright\"\n    )\n\n\n\n\n\n\nNote that UCSB is missing from the data. This could be because the census designates some universities as â€œgroup quartersâ€ and counts them differently compared to other residences. Furthermore, students are likely underrepresented in the 2020 census due to the COVID-19 pandemic and remote learning, as well as general confusion about current address vs.Â permanent address.\n\n\nVisualization 2: Party Proportions\n\n\nShow code\n# --------------- Visualization 2: Party Proportions Waffle Chart --------------\n# Create waffle chart of party distribution\nwaffle_counts &lt;- full_file %&gt;%\n  mutate(\n    party_code = if_else(\n      party_code %in% c(\"DEM\", \"REP\", \"NPP\", \"AI\"),\n      party_code,\n      \"Other\"\n    )\n  ) %&gt;%\n  count(party_code) %&gt;%\n  mutate(\n    percent = n / sum(n) * 100,\n    party_name = case_when(\n      party_code == \"DEM\" ~ \"Democrat\",\n      party_code == \"REP\" ~ \"Republican\",\n      party_code == \"NPP\" ~ \"No Party Preference\",\n      party_code == \"AI\" ~ \"American Independent\",\n      TRUE ~ \"Other\"\n    ),\n    party_label = paste0(party_name, \" (\", round(percent, 1), \"%)\"),\n    party_code = factor(\n      party_code,\n      levels = c(\"DEM\", \"REP\", \"NPP\", \"AI\", \"Other\")\n    )\n  ) %&gt;%\n  arrange(party_code)\n\n# Create named vector for legend labels\nparty_labels &lt;- setNames(\n    waffle_counts$party_label,\n    waffle_counts$party_code\n)\n\n# Update party colors to include AI\nparty_colors_updated &lt;- party_colors\nparty_colors_updated[\"AI\"] &lt;- \"gray40\"\n\n# Create waffle chart\nggplot(waffle_counts, aes(fill = party_code, values = n)) +\n  geom_waffle(\n    color = \"white\", \n    size = 2.5, \n    n_rows = 10,\n    make_proportional = TRUE\n  ) +\n  scale_fill_manual(\n    values = party_colors_updated,\n    labels = party_labels\n  ) +\n  coord_fixed() +\n  theme_voter_viz() +\n  theme(\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    axis.ticks = element_blank(),\n    axis.line = element_blank(),\n    legend.spacing.x = unit(1.0, \"cm\"),\n    legend.spacing.y = unit(1.0, \"cm\"),\n  ) +\n  labs(\n    title = \"Party Distribution in Santa Barbara County\",\n    fill = \"Party\"\n  )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n  \"party_waffle_chart.svg\",\n  path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n  width = 16,\n  height = 13\n)\n\n\n\n\nVisualization 3: Age Composition by Party\n\n\nShow code\n# -------------- Visualization 3: Age Composition by Party --------------\n# Calculate age distribution by party\nvoter_ages &lt;- full_file %&gt;%\n    mutate(\n        age_group = cut(\n            interval(dob, today()) %/% years(1),\n            breaks = c(18, 25, 35, 50, 65, Inf),\n            labels = c(\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\"),\n            right = FALSE\n        ),\n        party_code = factor(party_code, levels = c(\"DEM\", \"REP\"))\n    ) %&gt;%\n    filter(!is.na(party_code)) %&gt;%\n    count(party_code, age_group) %&gt;%\n    group_by(party_code) %&gt;%\n    mutate(pct = n / sum(n)) %&gt;%\n    ungroup()\n\n# Fetch and process Santa Barbara County census age data\nsb_age_data &lt;- get_acs(\n    geography = \"county\",\n    table = \"B01001\",\n    year = 2023,\n    cache_table = TRUE\n) %&gt;%\n    filter(NAME == \"Santa Barbara County, California\") %&gt;%\n    mutate(\n        age_group = case_when(\n            variable %in%\n                c(\"B01001_003\", \"B01001_004\", \"B01001_027\", \"B01001_028\") ~\n                \"18-25\",\n            variable %in%\n                c(\"B01001_005\", \"B01001_006\", \"B01001_029\", \"B01001_030\") ~\n                \"26-35\",\n            variable %in%\n                c(\n                    \"B01001_007\",\n                    \"B01001_008\",\n                    \"B01001_009\",\n                    \"B01001_031\",\n                    \"B01001_032\",\n                    \"B01001_033\"\n                ) ~\n                \"36-50\",\n            variable %in%\n                c(\n                    \"B01001_010\",\n                    \"B01001_011\",\n                    \"B01001_012\",\n                    \"B01001_013\",\n                    \"B01001_034\",\n                    \"B01001_035\",\n                    \"B01001_036\",\n                    \"B01001_037\"\n                ) ~\n                \"51-65\",\n            variable %in%\n                c(\n                    \"B01001_014\",\n                    \"B01001_015\",\n                    \"B01001_016\",\n                    \"B01001_017\",\n                    \"B01001_018\",\n                    \"B01001_019\",\n                    \"B01001_038\",\n                    \"B01001_039\",\n                    \"B01001_040\",\n                    \"B01001_041\",\n                    \"B01001_042\",\n                    \"B01001_043\"\n                ) ~\n                \"65+\",\n            TRUE ~ NA_character_\n        )\n    ) %&gt;%\n    filter(!is.na(age_group)) %&gt;%\n    group_by(age_group) %&gt;%\n    summarize(n = sum(estimate)) %&gt;%\n    mutate(\n        pct = n / sum(n),\n        party_code = \"SB County\"\n    ) %&gt;%\n    mutate(\n        age_group = factor(\n            age_group,\n            levels = c(\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\")\n        )\n    )\n\n# Combine data and create plot\nbind_rows(voter_ages, sb_age_data) %&gt;%\n    mutate(\n        party_code = factor(party_code, levels = c(\"DEM\", \"REP\", \"SB County\")),\n        label = paste0(age_group, \" (\", scales::percent(pct, accuracy = 1), \")\")\n    ) %&gt;%\n    ggplot(aes(x = party_code, y = pct, fill = age_group)) +\n    geom_col(width = 0.6) +\n    geom_text(\n        aes(label = label),\n        position = position_stack(vjust = 0.5),\n        color = \"black\",\n        fontface = \"bold\",\n        size = 5.5,\n        family = \"EBGaramond\"\n    ) +\n    scale_fill_brewer(palette = \"Purples\", direction = 1) +\n    scale_y_continuous(\n        labels = scales::percent,\n        breaks = seq(0, 1, 0.2)\n    ) +\n    labs(\n        title = \"Age Distribution of Voters in Santa Barbara County by Party\",\n        subtitle = \"Comparing party affiliations with overall county population\\n\",\n        x = NULL,\n        y = \"Proportion\",\n        caption = \"\\nSources: Santa Barbara Voter File (2025), US Census ACS 2023\"\n    ) +\n    theme_voter_viz() +\n    theme(\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = \"none\",\n        text = element_text(family = \"EBGaramond\")\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"age_composition_plot.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 13\n)\n\n\n\n\nVisualization 4: Registration Trends\n\n\nShow code\n# -------------- Visualization 4: Yearly Registration Trends --------------\n# Analyze registrations over time\nyearly_registration &lt;- full_file %&gt;%\n    mutate(year = format(registration_date, \"%Y\")) %&gt;%\n    filter(year &gt;= \"2000\") %&gt;%\n    filter(year &lt;= \"2024\") %&gt;%\n    count(year)\n\n# Create time series plot\nggplot(yearly_registration, aes(x = year, y = n)) +\n    geom_line(group = 1, color = \"purple4\") +\n    geom_point(color = \"purple4\") +\n    labs(\n        title = \"Voter Registrations in Santa Barbara County\",\n        subtitle = \"Number of new registrations from 2000 - 2024, showing spikes in new registrations during general election years\",\n        x = \"Year\",\n        y = \"Number of New Registrations\"\n    ) +\n    scale_x_discrete(\n        breaks = seq(\n            from = min(yearly_registration$year),\n            to = max(yearly_registration$year),\n            by = 4\n        )\n    ) +\n    theme(\n        panel.grid.major.y = element_blank(),\n        text = element_text(size = 24, family = \"EBGaramond\"),\n        axis.text = element_text(family = \"EBGaramond\"),\n        plot.title = element_text(face = \"bold\", family = \"EBGaramond\"),\n        plot.subtitle = element_text(family = \"EBGaramond\"),\n        plot.caption = element_text(family = \"EBGaramond\"),\n        legend.text = element_text(family = \"EBGaramond\")\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"yearly_registration_plot.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 13\n)\n\n\nIt seems obvious that more people register to vote in election years. But how long do they wait to register? Letâ€™s look at the 2024 March 5th primary election and November 5th presidential election to take a closer look at when people register to vote.\n\n\nShow code\n# Weekly trends code here\n# -------------- Visualization 4: Weekly Registration Trends --------------\n# Analyze 2024 registrations by week and party\nweekly_registrations &lt;- full_file %&gt;%\n    mutate(\n        reg_year = lubridate::year(registration_date),\n        reg_month = lubridate::month(registration_date),\n        reg_week = lubridate::week(registration_date),\n        reg_day = lubridate::day(registration_date),\n        week_start_date = lubridate::floor_date(registration_date, \"week\")\n    ) %&gt;%\n    filter(reg_year &gt;= 2024, reg_year &lt;= 2025) %&gt;%\n    count(reg_week, week_start_date, party_code) %&gt;%\n    group_by(reg_week, party_code) %&gt;%\n    slice(1) %&gt;%\n    ungroup() %&gt;%\n    filter(party_code %in% c(\"DEM\", \"REP\", \"NPP\"))\n\n# Create election day reference dates\nelection_dates &lt;- tibble(\n    date = as.Date(c(\"2024-03-05\", \"2024-11-05\")),\n    label = c(\"Primary\", \"General\")\n)\n\n# Create stacked area chart\nggplot(\n    weekly_registrations,\n    aes(x = week_start_date, y = n, fill = party_code)\n) +\n    geom_area(alpha = 0.8) +\n    geom_vline(\n        data = election_dates,\n        aes(xintercept = date),\n        linetype = \"dashed\",\n        color = \"red\"\n    ) +\n    labs(\n        title = \"Voter Registrations in Santa Barbara County (2024)\",\n        subtitle = \"Composition of new registrations by week and party\",\n        y = \"Number of New Registrations\",\n        fill = \"Party\"\n    ) +\n    scale_x_date(\n        date_breaks = \"1 month\",\n        date_labels = \"%b\" # Format: \"Jan 01\"\n    ) +\n    scale_fill_manual(values = party_colors) +\n    theme_voter_viz() +\n    theme(\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"weekly_registration_plot.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media\"),\n    width = 16,\n    height = 13\n)\n\n\n\n\nShow code\n# Daily trends code here\n# -------------- Visualization 4: Daily Registration Trends Before Election --------------\n# Analyze daily registrations for Oct 5 - Nov 5 period\ndaily_registrations &lt;- full_file %&gt;%\n    filter(\n        registration_date &gt;= as.Date(\"2024-10-05\"),\n        registration_date &lt;= as.Date(\"2024-11-05\"),\n        party_code %in% c(\"DEM\", \"REP\", \"NPP\")\n    ) %&gt;%\n    count(registration_date, party_code)\n\n# Count total number of registrations on key dates\ndaily_registrations_lookup &lt;- full_file %&gt;%\n  filter(\n    registration_date %in% c(\"2024-03-05\", \"2024-10-21\", \"2024-11-05\")\n  ) %&gt;%\n  count(registration_date)\n\n# Create reference dates\nreference_dates &lt;- tibble(\n    date = c(as.Date(\"2024-11-05\"), as.Date(\"2024-10-21\")),\n    label = c(\"Election Day\", \"Registration Deadline\")\n)\n\n# Create a stacked area chart with daily data\nggplot(\n    daily_registrations,\n    aes(x = registration_date, y = n, fill = party_code)\n) +\n    geom_area(alpha = 0.8) +\n    # Add vertical lines for both reference dates\n    geom_vline(\n        data = reference_dates,\n        aes(xintercept = date),\n        linetype = \"dashed\",\n        color = \"red\",\n    ) +\n    labs(\n        title = \"Voter Registrations in Santa Barbara County (Oct 5 - Nov 5, 2024)\",\n        subtitle = \"Daily registrations in the month leading up to the election\",\n        y = \"Number of New Registrations\",\n        fill = \"Party\"\n    ) +\n    scale_x_date(\n        breaks = seq.Date(\n            from = as.Date(\"2024-10-05\"),\n            to = as.Date(\"2024-11-05\"),\n            by = \"1 day\"\n        ),\n        labels = NULL,\n        date_minor_breaks = \"1 week\",\n        limits = c(as.Date(\"2024-10-05\"), as.Date(\"2024-11-05\"))\n    ) +\n    scale_y_continuous(\n        breaks = seq(0, 6200, by = 1000)\n    ) +\n    scale_fill_manual(values = party_colors) +\n    theme_voter_viz() +\n    theme(\n        axis.title.x = element_blank(),\n        legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\nShow code\nggsave(\n    \"daily_registration_plot.svg\",\n    path = here::here(\"posts/2025-03-17-voter-data-viz/media/\"),\n    width = 16,\n    height = 13\n)\n\n\nThe most amount of registrations occur on Election Day. California offers same-day voter registration for voters who need to vote provisionally (did not register in advance, moved addresses, voting at different polling location, etc.) This is not true for all states, however, especially those actively restricting voting rights."
  },
  {
    "objectID": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#discussion---design-elements-aesthetic-choices",
    "href": "posts/2025-03-17-voter-data-viz/SB-voter-visualization.html#discussion---design-elements-aesthetic-choices",
    "title": "Voter Registration Data Infographic",
    "section": "Discussion - design elements & aesthetic choices",
    "text": "Discussion - design elements & aesthetic choices\nSelect tab to see discussion on the topic.\n\nGraphic Form + General DesignText, Themes, Colors, & TypographyPrimary Message + ContributionAccessibilityDEI\n\n\nCreating my spatial visualization took the most amount of time, and I chose to include an interactive block group level map, because partisan differences can be obscured by geography and population. I wanted my quick-glance infographic to present the data in a formal, news-oriented format, as if it were something you would see on the New York Times or 538.\n\n\nMy aesthetic choices and theming were heavily influenced by New York Times and other legacy news media data visualizations. I chose red and blue since theyâ€™re the traditional colors that represent political parties, in addition to purple and grey as neutral colors. The text for my individual plots provides an overview for each plot as a standalone, explaining relationships and key takeaways.\n\n\nI aim to provide sub-county level visualizations of partisanship for SB County. While the south portion of the county is heavily Democratic, rural areas and north county leans more Republican. Registered Republicans are slightly older than Democrats, but also wait until the last minute to register to vote.\nThese visualizations provide a quick way to glean context from local outcomes of elections. They can be used as tool to compare vote shares for candiates or issues against, or to look at partisan counts for redistricting efforts.\n\n\nWhile my spatial visualization is not colorblind friendly, I have included alt-text to improve accessibility for those using screen readers. In the future, I would like to create a colorblind-friendly version of this infographic.\n\n\nBecause my voter data only includes age variables, I chose to focus on partisan differences between Democrats and Republicans in this analysis rather than incorporating other demographic variables. However, adding census-tract level analysis using additional variables like income, race/ethnicity, or housing burden from the American Community Survey could be interesting.\nFurthermore, a partisan analysis lends itself to the concepts of diversity, equity, and inclusion. The modern Republican party under Trump has attempted to kill DEI initiatives and mentions of it from government. Itâ€™s worth visualizing voter sentiments locally, because while Trump won the 2024 election, SB county remains deeply blue, especially south county."
  },
  {
    "objectID": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html",
    "href": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html",
    "title": "Casual Inference Study Recreation - Gatti et al, 2020",
    "section": "",
    "text": "Can Irrigation Infrastructure Mitigate the Effect of Rainfall Shocks on Conflict? uses data from Indonesia to provide evidence of an agricultural link between rainfall and conflict. Low rainfall during the agricultural season decreases production and increases civil conflict, specifically for conflicts about natural resources and popular justice. The researchers then show that the rainfall-conflict link is attenuated by the presence of irrigation infrastructure in a district.\nThe authors measured the total number of conflict incidents and the hyperbolic sine transformation of rice production in a given district-year as the outcome. The treatment variable was the interaction between rainfall and district-level irrigation infrastructure measured in 1997, before the study period.\nThey used a fixed-effects regression model to estimate the interaction between a districtâ€™s irrigation capacity and growing season rainfall in the current year, and used baseline irrigation capacity infrastructure in 1997 as an instrumental variable. The researchers chose this method because random assignment of infrastructure wasnâ€™t feasible and using predetermined irrigation capacity helped ensure that the observed effects were driven by weather conditions rather than other factors."
  },
  {
    "objectID": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#introduction",
    "href": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#introduction",
    "title": "Casual Inference Study Recreation - Gatti et al, 2020",
    "section": "",
    "text": "Can Irrigation Infrastructure Mitigate the Effect of Rainfall Shocks on Conflict? uses data from Indonesia to provide evidence of an agricultural link between rainfall and conflict. Low rainfall during the agricultural season decreases production and increases civil conflict, specifically for conflicts about natural resources and popular justice. The researchers then show that the rainfall-conflict link is attenuated by the presence of irrigation infrastructure in a district.\nThe authors measured the total number of conflict incidents and the hyperbolic sine transformation of rice production in a given district-year as the outcome. The treatment variable was the interaction between rainfall and district-level irrigation infrastructure measured in 1997, before the study period.\nThey used a fixed-effects regression model to estimate the interaction between a districtâ€™s irrigation capacity and growing season rainfall in the current year, and used baseline irrigation capacity infrastructure in 1997 as an instrumental variable. The researchers chose this method because random assignment of infrastructure wasnâ€™t feasible and using predetermined irrigation capacity helped ensure that the observed effects were driven by weather conditions rather than other factors."
  },
  {
    "objectID": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#recreation",
    "href": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#recreation",
    "title": "Casual Inference Study Recreation - Gatti et al, 2020",
    "section": "Recreation",
    "text": "Recreation\nIn this blog post we will be doing some exploration through the data data.dta associated with the paper.\nThe authors focus on two focal outcome variables: rice production & conflict incidents. For each variable they use different casual identification strategies to produce their data. For the focus of our analysis we will be working with conflict incidents, trying to recreate their model for Table 4 in R. They originally used Stata to produce their work so we will be doing our best to mimic the results as closely as possible.\nBased on what we could infer from the paper, our data.dta file, and the Stata documents, we will be focusing on the following variables.\n\nz_rgrowing_season_cm: rgrowing_season_cm z-scores\n\nrgrowing_season_cm: Rainfall during the growing season in centimeters\n\nz_tgrowing_season: tgrowing_season z-scores\n\ntgrowing_season: Temperature during the growing season\n\nz_rain_ground_dams_ha: Interaction between rainfall z-scores and irrigation capacity in 1997\nz_temp_ground_dams_ha: Interaction between temperature z-scores and irrigation capacity in 1997\nyear: Year\nprov: Province code\ndistrict_code: District\n\n\nVariables Representing Conflict Types:\n\nresource: Number of resource incidents\npop_justice: Number of popular justice incidents\nlaw_enf: Number of law enforcement incidents\ngov_prog: Number of government programs incidents\nseparatist: Number of separatist incidents\nidentity: Number of identity group incidents\n\n\n\nLoading Packages\n\n\nShow code\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(tidyr)\nlibrary(viridis)\nlibrary(forcats)\nlibrary(tools)\nlibrary(patchwork)\nlibrary(stringr)\nlibrary(here)\nlibrary(haven)\nlibrary(fixest)\nlibrary(interactions)\nlibrary(lfe)\nlibrary(stargazer)\n\n# Load study data\ndata_dta &lt;- read_dta(here::here(\"posts/2025-03-21-gatti-2020-recreation/data\", \"data.dta\"))\n\n\n\n\nData Exploration\nOne thing we noticed about .dta files is that in ours we have text underneath each of our column names explaining what the column represents. We canâ€™t extract this with just colnames() so we used a function to extract all those metadata labels.\n\n\nShow code\n# All column names + info about what it represents.\ncol_labels &lt;- sapply(data_dta, function(x) attr(x, \"label\"))\n\n# Show a few of our column names + metadata.\nhead(col_labels)\n\n\n$district_code\n[1] \"District\"\n\n$prov\n[1] \"Province code\"\n\n$island_code\n[1] \"Island\"\n\n$island_2\n[1] \"Outer and Inner islands\"\n\n$year\n[1] \"Year\"\n\n$total_area\n[1] \"Rice area in hectares\"\n\n\n\n\nExplore Conflict Types\nFrom our variables we noticed 6 different conflict types which were resource, pop_justice, law_enf, gov_prog, separatist, identity that we should take a look into.\nBelow is a visualizations of all the conflict types and their # of reported incidents.\n\n\nShow code\n# Filter out data for specific conflict types and find sum of reported incidents.\nsubset_of_conflicts &lt;- data_dta %&gt;%\n    select(c(\n        resource,\n        gov_prog,\n        separatist,\n        identity,\n        pop_justice,\n        law_enf\n    )) %&gt;%\n    pivot_longer(\n        cols = everything(),\n        names_to = \"Conflict_Type\",\n        values_to = \"Count\"\n    ) %&gt;%\n    group_by(Conflict_Type) %&gt;%\n    summarise(Total_Count = sum(Count, na.rm = TRUE)) %&gt;%\n    arrange(desc(Total_Count)) %&gt;%\n    ungroup()\n\n# Clean up variable names.\nsubset_of_conflicts &lt;- subset_of_conflicts %&gt;%\n    mutate(\n        Conflict_Type = case_when(\n            Conflict_Type == \"gov_prog\" ~ \"Government Program\",\n            Conflict_Type == \"identity\" ~ \"Identity Conflict\",\n            Conflict_Type == \"law_enf\" ~ \"Law Enforcement\",\n            Conflict_Type == \"pop_justice\" ~ \"Popular Justice\",\n            Conflict_Type == \"resource\" ~ \"Resource Conflict\",\n            Conflict_Type == \"separatist\" ~ \"Separatist Movement\",\n            TRUE ~ Conflict_Type # Keep unchanged if not listed.\n        ),\n        Conflict_Type = fct_reorder(Conflict_Type, Total_Count)\n    )\n\n# Plot Conflict Types by # of incidents.\nggplot(\n    subset_of_conflicts,\n    aes(x = Conflict_Type, y = Total_Count, fill = Conflict_Type)\n) +\n    geom_bar(stat = \"identity\") +\n    labs(\n        x = \"Conflict Types\",\n        y = \"# of Incidents\",\n        title = \"Distribution of Conflict Types\"\n    ) +\n    scale_fill_viridis(discrete = TRUE, option = \"plasma\") +\n    coord_flip() +\n    theme_minimal() +\n    theme(\n        legend.position = \"none\",\n        axis.title.y = element_blank(),\n        panel.grid.minor = element_blank(),\n    )\n\n\n\n\n\n\n\n\n\nPopular Justice had the highest number of incidents, just under 12,000. Separatist Movement followed with around 8,000, and Law Enforcement had about 6,000. The last three (Resource Conflict, Identity Conflict, and Government Program) together make up roughly the same number of incidents as the Separatist Movement. In the study, the authors found the strongest link between their treatment effect and conflicts over natural resources, followed by popular justice and law enforcement, which makes sense given that they are investigating agricultural production.\nMoving on, letâ€™s start setting up our models for Table 4.\n\n\nFilter Data by Model Variables\nFor our data we want to limit it to the variables we previously outlined as we prep for our model.\nWe want to filter out for only district, year, province, conflict types, (standardized) temperature, (standardized) irrigation capacity, (standardized) rainfall as they are all necessary for reproducing our desired table.\n\n\nShow code\n# List of our conflict type column names.\nconflict_vars &lt;- c(\n    \"resource\",\n    \"pop_justice\",\n    \"law_enf\",\n    \"gov_prog\",\n    \"separatist\",\n    \"identity\"\n)\n\ndata_dta &lt;- data_dta %&gt;%\n    select(c(\n        district_code,\n        prov,\n        year,\n        z_rgrowing_season_cm,\n        z_tgrowing_season,\n        z_rain_ground_dams_ha,\n        z_temp_ground_dams_ha,\n        all_of(conflict_vars)\n    ))\n\n\n\n\nSetting up our Fixed-Effects Regression by Conflict Type\nTo recreate our table from our model its important to try and take a look at the Stata code they used to build theirs.\nTheir table code looked something like this:\nxtreg `var' `rain1' `temp1' `irrigation' \ni.year \nc.year#i.prov \ni.prov#c.year, \nfe vce(cluster district_code)\n    outreg2 using table4, word excel append keep(\nz_rgrowing_season_cm c.z_ground_dams_ha#\nc.z_rgrowing_season_cm) nocons\nThis is what their table looked like:\n\n\n\nImpact of Growing-Season Rainfall by Subcategories of Conflict: Table 4\n\n\nFrom their code we can see some aspects on how they set up their table. We can see that they use var rain1 temp1 irrigation which â€œvarâ€ presumably represents the conflict type, â€œrain1â€ represents our growing season rainfall, â€œtemp1â€ represents our growing season temp, and lastly â€œirrigationâ€ is our irrigation capacity.\nWe can also point out the following that,\n\ni.year: represents year fixed effects,\nc.year#i.prov: Year-province interactions to account for province-specific trends over time,\nfe vce(cluster district_code): District-level fixed effects with clustered standard errors.\n\nThey only want to view the results for the growing season rainfall as well as the interaction it has with the irrigation capacity.\nWe also have an excerpt from our paper which states:\nâ€œNote: All regressions include temperature and temperature interacted with irrigation capacity, as well as district fixed effects, province-specific linear time trends, and year fixed effects. Irrigation is defined as irrigation capacity in 1997, the year before the start of the period of observation. Growing-season rainfall is the sum of rainfall from November in tâˆ’ 1 to April in year t. The outcomes are the number of conflict incidents by type of conflict. Standard errors, clustered at the district level, are in parentheses, where ***, **, and * denote statistical significance at the 1%, 5%, and 10% levels, respectively.â€\nFrom this we can piece together our model based on the variables weâ€™ve extracted from our set.\nOur equation is going to look like this:\n\\[\\tiny \\begin{align*} conflict\\_type_{i,t} = & \\ \\beta_0 + \\beta_1 z\\_rgrowing\\_season\\_cm_{i,t} + \\beta_2 z\\_rain\\_ground\\_dams\\_ha_{i,t} \\\\ & + \\beta_3 z\\_tgrowing\\_season_{i,t} + \\beta_4 z\\_temp\\_ground\\_dams\\_ha_{i,t} \\\\ & + \\gamma_t + \\delta_t \\cdot prov_i + \\alpha_i + \\varepsilon_{i,t} \\end{align*}\\]\n\n\nShow code\n# Function for running fixed-effect regressions used in Table 4 for each conflict type.\nrun_regression &lt;- function(var) {\n    feols(\n        as.formula(paste0(\n            var,\n            \" ~ z_rgrowing_season_cm + z_rain_ground_dams_ha +\n                       z_tgrowing_season + z_temp_ground_dams_ha +\n                       factor(year) + year:factor(prov) | district_code\"\n        )),\n        data = data_dta,\n        cluster = ~district_code\n    )\n}\n\n# Run our regression formula for each of our conflict types.\nconflict_models &lt;- lapply(conflict_vars, run_regression)\n\n\n\nAdding significance indicators to our model coefficients\n\n\nShow code\n# Function to extract regression coefficients and add significance stars.\nextract_coefficients &lt;- function(model) {\n    coefs &lt;- coef(model)\n    std_errors &lt;- sqrt(diag(vcov(model)))\n    p_values &lt;- 2 * (1 - pnorm(abs(coefs / std_errors)))\n\n    data.frame(\n        term = names(coefs),\n        value = paste0(\n            round(coefs, 3),\n            case_when(\n                p_values &lt; 0.01 ~ \"***\",\n                p_values &lt; 0.05 ~ \"**\",\n                p_values &lt; 0.1 ~ \"*\",\n                TRUE ~ \"\"\n            ),\n            \" &lt;br&gt;(\",\n            round(std_errors, 3),\n            \")\"\n        )\n    )\n}\n\n# Add significance indicator for all models.\nresults_list &lt;- lapply(conflict_models, extract_coefficients)\n\n\n\n\nExtracting other relevant model statistics for Table4\n\n\nShow code\n# Function to calculate given statistical function across all conflict models.\nextract_model_stat &lt;- function(statistic, name) {\n    data.frame(\n        term = name,\n        matrix(sapply(conflict_models, function(x) statistic(x)), nrow = 1)\n    ) %&gt;%\n        mutate(across(-term, as.character))\n}\n\n# Extract R_Squared, # of districts, and # of observations.\nr_squared_df &lt;- extract_model_stat(\n    function(x) round(r2(x, type = \"wr2\"), 3),\n    \"R_Squared\"\n)\nnum_districts_df &lt;- extract_model_stat(\n    function(x) length(fixef(x)$district_code),\n    \"Num. Districts\"\n)\nnum_obs_df &lt;- extract_model_stat(nobs, \"Num. Observations\")\n\n\n\n\nRenaming our columns to display appropriately in our resulting table.\nIn the last step we calculated the R_Squared, # Obvs, and # of Districts per our results for each model by conflict type.\nSince our dataframes share a similar structure we need to make sure to clean all of their column names as we get them ready to merge into the final table.\n\n\nShow code\n# Renaming our conflict variables for our table.\nconflict_labels &lt;- c(\n    \"gov_prog\" = \"Government&lt;br&gt;Program\",\n    \"identity\" = \"Identity\",\n    \"law_enf\" = \"Law&lt;br&gt;Enforcement\",\n    \"pop_justice\" = \"Popular&lt;br&gt;Justice\",\n    \"resource\" = \"Resource\",\n    \"separatist\" = \"Separatist&lt;br&gt;Movement\"\n)\n\n# Function to rename conflict type variables with their appropriate names.\nrename_cols &lt;- function(df) {\n    colnames(df) &lt;- c(\"term\", conflict_labels[conflict_vars])\n    return(df)\n}\n\n# Renaming the columns in these dataframes to match when we merge them later.\nr_squared_df &lt;- rename_cols(r_squared_df)\nnum_districts_df &lt;- rename_cols(num_districts_df)\nnum_obs_df &lt;- rename_cols(num_obs_df)\n\n\n\n\nMerge conflict model results into a data frame\nIn this step we compile our results into a neat table for our conflict types and rename the output terms to the ones specified in Table 4 from the paper.\n\n\nShow code\n# Rename regression terms for our table.\nterm_labels &lt;- c(\n    \"z_rgrowing_season_cm\" = \"GS Rainfall\",\n    \"z_rain_ground_dams_ha\" = \"GS Rainfall Ã— Irrigation Capacity\"\n)\n\n# Convert results into a single dataframe and clean up output variable labels.\nresults_df &lt;- bind_rows(results_list, .id = \"Conflict_Type\") %&gt;%\n    filter(term %in% names(term_labels)) %&gt;%\n    mutate(term = term_labels[term]) %&gt;% # Clean up term labels.\n    pivot_wider(names_from = \"Conflict_Type\", values_from = \"value\") %&gt;%\n    rename_with(~ conflict_labels[conflict_vars], -term) # Rename col names to match the rest.\n\n\n\n\nAppending our R_Squared, District Count, and Observation Counts\nIn this last step we finally get to merge all of our resulting dataframes into one as they all share the same column names.\nHere we finalize our table using the kableExtras package.\n\n\nShow code\n# Bind r_squared, district counts, and observations to the table.\nfinal_results &lt;- bind_rows(\n    results_df,\n    r_squared_df,\n    num_districts_df,\n    num_obs_df\n)\n\n# Format table using kable.\nfinal_results %&gt;%\n    kable(\n        format = \"html\",\n        caption = \"Table 4: Impact of Growing-Season Rainfall on Conflict Subcategories\",\n        col.names = c(\"Variable\", conflict_labels[conflict_vars]),\n        escape = FALSE\n    ) %&gt;%\n    kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\"))\n\n\n\nTable 4: Impact of Growing-Season Rainfall on Conflict Subcategories\n\n\n\n\n\n\n\n\n\n\n\nVariable\nResource\nPopular\nJustice\nLaw\nEnforcement\nGovernment\nProgram\nSeparatist\nMovement\nIdentity\n\n\n\n\nGS Rainfall\n-0.108***\n(0.04)\n-0.317***\n(0.105)\n-0.292***\n(0.07)\n-0.058**\n(0.026)\n0.342\n(0.228)\n0.078\n(0.048)\n\n\nGS Rainfall Ã— Irrigation Capacity\n0.018***\n(0.005)\n0.068*\n(0.035)\n0.014\n(0.009)\n-0.004\n(0.003)\n-0.014\n(0.029)\n0.023**\n(0.009)\n\n\nR_Squared\n0.217\n0.232\n0.227\n0.269\n0.223\n0.091\n\n\nNum. Districts\n201\n201\n201\n201\n201\n201\n\n\nNum. Observations\n3417\n3417\n3417\n3417\n3417\n3417\n\n\n\n\n\n\n\nWe can see that rainfall is generally reducing our conflict types with the exception of separtist and identity conflicts.\nBased on our second variable we can see that irrigation capacity makes rainfall more likely to effect the following conflicts: natural resource, popular justice, and law enforcement but mitigates our other three, which follows the results from the paper.\nWe can see that rainfall, irrigation, and their interaction explain a meaningful part of the variation in conflict incidents of around 20% with the exception of Identity Conflicts at 9%.\n\n\n\nChecking out the interactions plot\n\n\nShow code\n# Create an empty list to hold our model plots.\ninteraction_plots &lt;- list()\n\n# Loop through each model and generate their respective interaction plot.\nfor (i in seq_along(conflict_models)) {\n    conflict_model &lt;- conflict_models[[i]] # Pulling each model by type.\n    conflict_type &lt;- conflict_vars[i] # Conflict variable name.\n\n    # Generate predicted values.\n    pred_data &lt;- data_dta %&gt;%\n        mutate(predicted = predict(conflict_model, newdata = data_dta)) %&gt;%\n        select(z_rgrowing_season_cm, z_rain_ground_dams_ha, predicted)\n\n    # Convert to a Simplified version to fit as an lm object\n    # because interact_plot doesn't work with `Fixest` package.\n    conflict_model_lm &lt;- lm(\n        predicted ~ z_rgrowing_season_cm * z_rain_ground_dams_ha,\n        data = pred_data\n    )\n\n    # Format conflict_type names for our plot.\n    # Our conflict_labels had html which doesn't work in our plot. Easier to do this.\n    conflict_names &lt;- toTitleCase(str_replace_all(conflict_type, \"_\", \" \"))\n\n    # Plot interaction.\n    conflict_interaction_plot &lt;- interact_plot(\n        conflict_model_lm,\n        pred = z_rgrowing_season_cm,\n        modx = z_rain_ground_dams_ha,\n        interval = TRUE,\n        legend.main = \"Irrigation Capacity\" # custom legend name.\n    ) +\n        ggtitle(paste(\"Interaction for\", conflict_names)) +\n        theme_minimal() +\n        labs(\n            x = \"Growing Season Rainfall (cm)\",\n            y = \"Predicted Conflict Incidents \",\n        )\n\n    # Add plot to our plot list for patchwork.\n    interaction_plots[[i]] &lt;- conflict_interaction_plot\n}\n\n# Combine all plots into a single figure.\ncombined_plot &lt;- wrap_plots(interaction_plots, ncol = 2)\n\n# All axis titles are the same we can make patchwork clean up our labels.\ncombined_plot +\n    plot_annotation(title = \"Rainfall & Irrigation on Conflict Types\") + # patchwork title.\n    plot_layout(\n        axis_titles = \"collect\", # 1x 1y axis label.\n        guides = \"collect\"\n    ) # 1 legend."
  },
  {
    "objectID": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#citations",
    "href": "posts/2025-03-21-gatti-2020-recreation/gatti-2020-recreation.html#citations",
    "title": "Casual Inference Study Recreation - Gatti et al, 2020",
    "section": "Citations",
    "text": "Citations\nGatti, N., Baylis, K. and Crost, B. (2020), Can Irrigation Infrastructure Mitigate the Effect of Rainfall Shocks on Conflict? Evidence from Indonesia. Amer. J. Agr. Econ., 103: 211-231. https://doi.org/10.1002/ajae.12092"
  },
  {
    "objectID": "deletelater/practice.html",
    "href": "deletelater/practice.html",
    "title": "Here is my level one header",
    "section": "",
    "text": "Here is my level one header\nHere is my first paragraph\nHere is my second paragraph, where you can read more about MEDS.\nThis is very important text!"
  },
  {
    "objectID": "portrait-photography.html#portraits",
    "href": "portrait-photography.html#portraits",
    "title": "Portrait Photography",
    "section": "Portraits",
    "text": "Portraits"
  },
  {
    "objectID": "portrait-photography.html#graduates",
    "href": "portrait-photography.html#graduates",
    "title": "Portrait Photography",
    "section": "Graduates",
    "text": "Graduates"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Casual Inference Study Recreation - Gatti et al, 2020\n\n\n\n\n\n\nR\n\n\nStatistical Analysis\n\n\nCausal Inference\n\n\nAgriculture\n\n\n\nRecreating Results from Gatti et al, 2020\n\n\n\n\n\nMar 21, 2025\n\n\nTakeen Shamloo, Karol Paya, Leilanie Rubinstein\n\n\n\n\n\n\n\n\n\n\n\n\nVoter Registration Data Infographic\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\nSpatial Analysis\n\n\nPolitical Data\n\n\n\nVisualizing partisan trends using Santa Barbara County voter registration data\n\n\n\n\n\nMar 17, 2025\n\n\nLeilanie Rubinstein\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire: Perimeter Visualization & Land Cover Statistics\n\n\n\n\n\n\nPython\n\n\nMEDS\n\n\n\nTrue and false color visualization using Landsat data\n\n\n\n\n\nDec 13, 2024\n\n\nLeilanie Rubinstein\n\n\n\n\n\n\n\n\n\n\n\n\nOcean Chemistry and Santa Barbara Channel Macrocystis Pyrifera Populations\n\n\n\n\n\n\nR\n\n\nMarine\n\n\nStatistical Analysis\n\n\n\nInvestigating whether ocean chemistry has an impact on kelp forest biomass in the Santa Barbara Channel\n\n\n\n\n\nDec 12, 2024\n\n\nLeilanie Rubinstein\n\n\n\n\n\n\n\n\n\n\n\n\nAquaculture Suitability Map\n\n\n\n\n\n\nR\n\n\nMarine\n\n\nGeospatial Analysis\n\n\n\nDetermining and visualizing which West Coast EEZs are best suited to developing species-specific marine aquaculture\n\n\n\n\n\nDec 11, 2024\n\n\nLeilanie Rubinstein\n\n\n\n\n\n\n\n\n\n\n\n\nHouston Blackouts Mapping\n\n\n\n\n\n\nR\n\n\nSpatial Analysis\n\n\nRemote Sensing\n\n\nEnergy\n\n\n\nIdentifying impacts of extreme weather on the Texas power grid\n\n\n\n\n\nDec 9, 2024\n\n\nLeilanie Rubinstein\n\n\n\n\n\n\nNo matching items"
  }
]